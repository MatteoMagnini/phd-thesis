%! Author = matteomagnini
%! Date = 05/03/25

%----------------------------------------------------------------------------------------
\chapter{NeSy AI for real world applications}
\label{ch:nesy-ai-for-real-world-applications}

\begin{flushright}
\begin{minipage}{0.5\textwidth}
    How to apply \gls{SKI} and \gls{SKE} methods to design real-world \gls{AI} systems?

    How can \glspl{LLM} be exploited in the context of \gls{SKI} and \gls{SKE}?

    -- \textbf{\Cref{itm:rq0,itm:rq3,itm:rq4}}
\end{minipage}
\end{flushright}

\mtcaddchapter
\minitoc
%----------------------------------------------------------------------------------------


%
In this chapter we present a selection of contributions about \gls{NeSy} application in real-world scenarios.
%
These works involve the use of both \gls{SKE} and \gls{SKI} methods, as well as traditional \gls{ML} models and the recent \glspl{LLM}.


\section{\Gls{SKE} for explainable nutritional recommenders}\label{sec:ske-for-explainable-nutritional-recommenders}
%
``Symbolic knowledge extraction for explainable nutritional recommenders''~\cite{DBLP:journals/cmpb/MagniniCCAO23}, published on the Journal of Computer Methods and Programs in Biomedicine, presents a novel nutritional recommender system that leverages \gls{SKE}.


\subsection{Nutritional recommender systems}\label{subsec:nutritional-recommender-systems}

Eating habits significantly impact the well-being of individuals across all age groups, highlighting the importance of developing nutritional \glspl{RS}~\cite{EspinHN2016,Cioara2018,Shandilya2022}.
%
These systems address diverse user needs, including diet programs, chronic disease management, treatment for critically ill patients, allergies, lifestyle choices (e.g., sporty, vegetarian, organic, halal), and physical activity levels~\cite{Saiz2021,Hezarjaribi2019,AgapitoSCCLGPFC2018,FraserT1990,Tran2018}.
%
User preferences are represented either by expert knowledge, such as daily nutritional intake limits based on physical activity levels, or by user feedback, such as reviews on recipes~\cite{10.1145/3418211}.
%
Recommendation items, such as food, recipes, and meals, are represented in terms of their attributes, including cuisine, category, cooking style, preparation time, and nutritional levels.
%
The complexity of nutritional \glspl{RS} arises from the combination of multiple ingredients to form recipes and the diverse attributes influencing user preferences.

Classical approaches to nutritional recommendation rely on content based and collaborative filtering methods, which derive user profiles from past activities, such as ratings, clicks, reviews, and browsing history~\cite{Min2020}.
%
Recent advancements leverage \gls{ML} techniques, including question answering over knowledge bases, recipe retrieval from visual records of ingredients, and learning recipe representations from multi-modal data~\cite{Forbes2011,Bianchini2017,Freyne2010,Chen2021,Tian2022}.
%
Despite the availability of recipes online, many are suboptimal in terms of health~\cite{Trattner2017a}.
%
Recent studies aim to promote healthy eating by incorporating healthiness indicators into recommendations or enhancing the visual appeal of food~\cite{Saiz2021,10.1145/3418211}.
%
Providing explanations for recommendations improves user trust and acceptance, as transparent systems are preferred over black-box models~\cite{Anjomshoae2019}.
%
Explainable approaches, such as explanation ontologies and multi-agent architectures, have been proposed to enhance both personalisation and explainability in nutritional \glspl{RS}~\cite{DBLP:conf/icde/PadhiarSCGM21,Yera2022,expectation-extraamas2021}.

\subsection{Methodology}\label{subsec:methodology}
%
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/ske-recommender-workflow}
  \caption[Data-flow perspective of a nutritional recommender system]{%
    Data-flow perspective of a nutritional \gls{RS}.
    %
    The user interacts -- via some smart/wearable device -- with a sub-symbolic AI predictor, continuously trained to predict whether the user likes a given recipe or not.
    %
    A knowledge base is then extracted from the predictor, representing the user's preferences in a human-interpretable logic form.
    %
    Dietary prescriptions -- provided by human experts -- are transformed into the same logic form.
    %
    Conversely, databases providing information about recipies -- there including ingredients and their nutrients -- are assumed to be remotely available via the Internet.
    %
    Finally, the recommending agent exploits a logic engine, combining all such information into recommendations which are simultaneously correct (w.r.t.\ experts prescriptions), acceptable (w.r.t.\ users' preferences), and explainable.
  }
  \label{fig:ske-recommender-workflow}
\end{figure}

We propose a general architecture for nutritional \glspl{RS}, designed to provide personalised dietary recommendations aligned with specific user goals.
%
\Cref{fig:ske-recommender-workflow} illustrates the main components and data flow within the system.
%
The proposed architecture personalises dietary recommendations by identifying recipes that lie at the intersection of user preferences and expert prescriptions.
%
This process relies on three distinct sources of information:
\begin{inlinelist}
    \item the user,
    \item the nutrition expert, and
    \item the recipe database.
\end{inlinelist}
%
Users provide their preferences, which are learned adaptively by a sub-symbolic \gls{ML} predictor.
%
Nutrition experts translate dietary goals into structured prescriptions, which define patterns of recipes suitable for achieving these goals.
%
The recipe database stores information about recipes, their ingredients, and corresponding nutrients, enabling the agent to propose admissible options.
%
Recommendations are generated by combining these sources using a logic-based approach, ensuring alignment with both user preferences and expert prescriptions.
%
In the following sections, we detail how preferences, prescriptions, and recipes are represented and manipulated within the system.
%
We also describe the algorithmic process for producing recommendations and discuss its underlying assumptions.


\paragraph{Recipes}\label{par:recipes}
%
Our architecture identifies three key data types: recipes, ingredients, and nutrients.
%
Recipes are composed of ingredients, which in turn contain various nutrients.
%
Each data type is named, meaning recipes, ingredients, and nutrients are uniquely identified by their names.
%
The architecture does not track the preparation process of meals but focuses on the nominal composition of ingredients and their quantities.
%
By analyzing the composition of ingredients in terms of nutrients, the system computes the overall nutritional values of recipes.
%
The recipes' database is responsible for storing information about recipes, ingredients, and nutrients.
%
It supports several types of queries, such as:
%
\begin{itemize}
  \item Selecting recipes based on their ingredients or nutrients.
  %
  \item Filtering recipes by specific quantities of ingredients or nutrients.
  %
  \item Retrieving the ingredients or nutrients associated with a recipe.
  %
  \item Clustering recipes with similar nutritional profiles.
\end{itemize}
%
These queries are essential for the recommender agent's functionality.


\paragraph{Users' Preferences}\label{par:users-preferences}
%
User preferences are represented sub-symbolically using a \gls{ML} predictor.
%
This predictor learns user tastes adaptively from data, which includes information about recipes the user likes or dislikes.
%
Preferences are modeled as a function:
%
\begin{equation}
  \label{eq:users-preferences}
  \text{appreciation}: R \to \mathbb{R}
\end{equation}
%
where \( R \) is the set of admissible recipes, and the output is an appreciation score.
%
Positive values indicate liking, negative values indicate dislike, and zero represents neutrality.
%
The appreciation score encapsulates user opinions, including factors such as taste, allergies, and ethical beliefs.
%
Data is collected during user interactions, often via smart or wearable devices, and used to train the \gls{ML} predictor.
%
The learning process is continual, ensuring the system adapts to evolving user preferences.
%
The sub-symbolic approach enables generalization of user data while leveraging information from the recipes' database.


\paragraph{Dietary Prescriptions}\label{par:dietary-prescriptions}
%
Dietary prescriptions are structured representations of what a user should eat and when, to achieve specific goals.
%
They are typically created by nutrition experts based on the user's physiological characteristics and expert knowledge.
%
Prescriptions consist of two components:
%
\begin{itemize}
  \item The \emph{what} component specifies ingredients or nutrients and their quantities for each meal.
  %
  \item The \emph{when} component indicates the time of consumption (e.g., breakfast, lunch, dinner).
\end{itemize}
%
The goal reflects the expected long-term effect on the user's health, though it may not be explicitly stated.
%
Prescriptions are often provided in quasi-natural language or tabular formats.
%
In tabular form, each cell corresponds to a specific time and meal, listing the recommended nutrients or ingredients.
%
Users may construct recipes based on these suggestions unless a nutritional \gls{RS} is available.
%
For simplicity, we focus on a single prescription for a given time \( t \), expressed as logic formulas.
%
These formulas define the properties a meal should have to align with dietary goals.
%
Details on the use of logic formulas for dietary prescriptions are provided in the next subsection.


\paragraph{The role of logic formulas}\label{par:the-role-of-logic-formulas}

The proposed architecture utilizes Horn clauses to represent both user preferences and expert prescriptions.
%
Horn logic provides a clear and computationally tractable framework for expressing dietary prescriptions.
%
These formulas can be manually written by humans or algorithmically generated, enabling their use in \glspl{RS} for dietary recommendations.

%
Expert prescriptions at time \( t \) are defined as a set of Horn clauses describing the \texttt{should\_eat/1} predicate.
%
This predicate specifies the recipes that the user should consume, based on admissible or forbidden ingredients and nutrients.
%
Two auxiliary predicates, \texttt{has/2} and \texttt{has\_no/2}, are used to assert the presence or absence of specific ingredients or nutrients.
%
Groups of ingredients or nutrients can also be defined using unary predicates, such as \texttt{vegetable/1}.

%
For example, a dietary prescription for Monday lunch can be expressed as:
%
\begin{align*}
  \begin{array}{rcl}
    \texttt{should\_eat}(R) &\leftarrow& \texttt{has}(R, \texttt{rice}) \land \texttt{has}(R, \texttt{chicken}) \\
                            &\land& \texttt{has\_no}(R, \texttt{salt}) \\
                            &\land& \texttt{has}(R, X) \land \texttt{vegetable}(X)
  \end{array}
\end{align*}
%
indicating that the user should consume recipes containing rice, chicken, and any vegetable, but excluding salt.

%
To ensure self-containment, additional rules must define the semantics of predicates such as \texttt{has/2}, \texttt{has\_no/2}, and \texttt{vegetable/1}.
%
For brevity, these definitions are omitted here.

%
Alternative encodings, such as including a predicate \texttt{quantity(X, Q)} to specify the quantity \( Q \) of ingredient \( X \), are possible but do not alter the core contribution.
%
Thus, simpler syntactic choices are preferred.

%
User preferences can also be represented as Horn clauses, defining the \texttt{likes/1} predicate.
%
These clauses may use the same auxiliary predicates (\texttt{has/2}, \texttt{has\_no/2}) and custom predicates for grouping ingredients or nutrients.

%
Consider the following example:
%
\begin{align*}
  \begin{array}{rcl}
    \texttt{likes}(R) &\leftarrow& \texttt{has}(R, \texttt{rice}) \land \texttt{has}(R, \texttt{chicken}) \\
                      &\land& \texttt{has\_no}(R, \texttt{broccoli}) \land \texttt{has}(R, \texttt{peas}) \\
    \texttt{vegetable}(\texttt{peas}) &\leftarrow \\
    \texttt{vegetable}(\texttt{broccoli}) &\leftarrow \\
    \texttt{has}(\texttt{paella}, \texttt{rice}) &\leftarrow \\
    \texttt{has}(\texttt{paella}, \texttt{chicken}) &\leftarrow \\
    \texttt{has}(\texttt{paella}, \texttt{peas}) &\leftarrow \\
    \texttt{has}(\texttt{paella}, \texttt{seafood}) &\leftarrow
  \end{array}
\end{align*}
%
This example states that the user likes recipes containing rice, chicken, and peas (e.g., paella), but dislikes broccoli.
%
It also defines peas and broccoli as vegetables and specifies the composition of paella.

%
When both prescriptions and preferences are expressed as Horn clauses, their intersection can be computed using logic resolution.
%
This involves proving the query:
%
\begin{equation}
    \label{eq:intersection-prescriptions-preferences}
    \texttt{likes}(R) \land \texttt{should\_eat}(R)
\end{equation}
%
against the merged clause set of prescriptions and preferences.
%
The logic solver identifies recipes \( R \) that satisfy both conditions or determines that no such recipes exist.

%
For instance, testing the query above against the merged clauses from the examples provided yields \( R = \texttt{paella} \).
%
This result is denoted as:
%
\begin{equation}
  \label{eq:intersection-prescriptions-preferences-final}
  (3) \cup (4) \models \texttt{likes}(\texttt{paella}) \land \texttt{should\_eat}(\texttt{paella})
\end{equation}


\paragraph{The role of \gls{SKE}}\label{par:the-role-of-ske}
%
Our proposed architecture requires both user preferences and expert prescriptions to be expressed as sets of Horn clauses.
%
This representation enables the use of logic resolution to construct recommendations.
%
Expert prescriptions are typically provided in formats, such as timetables of suggested recipes, that can be directly expressed or automatically converted into Horn clauses~\cite{DBLP:journals/jcss/Makowsky87}.
%
This assumption aligns with current practices in dietary planning.
%
In contrast, user preferences are modeled using sub-symbolic predictors, such as trained \glspl{NN}, which adaptively learn preferences from data.
%
While sub-symbolic representations are effective for capturing dynamic user preferences, they are incompatible with direct logic resolution.
%
To bridge this gap, the architecture incorporates a \gls{SKE} step.
%
This step extracts symbolic knowledge in the form of Horn clauses from the sub-symbolic predictor trained to model user preferences.
%
The choice of \gls{SKE} algorithm is left to the implementer, allowing flexibility to select the most suitable method for their application.
%
However, the extraction process must produce Horn clauses to ensure compatibility with the logic-based recommendation framework.
%
This approach combines the adaptability of sub-symbolic models with the interpretability and computational efficiency of symbolic reasoning.


\subsection{Validation}\label{subsec:validation-ske-nutrition}
%
To validate the proposed architecture, we conducted a series of experiments to assess its effectiveness in nutritional \glspl{RS}.
%
The source code and instructions for reproducing these experiments are public available\footnote{\url{https://github.com/pikalab-unibo/mccao-cmpb-experiments-2022}}
%
The experiments involved four main steps:
%
\begin{enumerate}
  \item Generating synthetic datasets to simulate a single user's food preferences.
  %
  \item Training a \gls{ML} predictor, specifically a neural network, to predict whether a recipe would be liked by the user.
  %
  \item Applying a \gls{SKE} algorithm to extract symbolic knowledge that represents the decision-making behavior of the predictor.
  %
  \item Evaluating the system's ability to recommend recipes that align with both user preferences and expert nutritional prescriptions.
\end{enumerate}
%
Details regarding data selection, synthesis, and experimental procedures are provided in the following subsections.

\paragraph{Datasets}\label{par:datasets-ske-nutrition}
%
We utilized a public dataset of recipes\footnote{available at \url{https://cosylab.iiitd.edu.in/culinarydb}} and generated 12 synthetic datasets to represent the preferences of imaginary users.
%
The recipe dataset consists of four files:
%
\begin{itemize}
  \item \textbf{Recipe Details:} Contains recipe IDs, titles, sources, and cuisines.
  %
  \item \textbf{Ingredients:} Lists basic ingredients with aliases, synonyms, entity IDs, and categories.
  %
  \item \textbf{Compound Ingredients:} Includes compound ingredients with their constituent components and categories.
  %
  \item \textbf{Recipe-Ingredients Aliases:} Maps recipes to their ingredients using aliases and entity IDs.
\end{itemize}
%
The dataset includes 929 unique basic ingredients and 103 compound ingredients, categorized into 21 groups (e.g., additive, bakery, beverage).
%
Recipes with at least one ingredient total 45,749, while recipes without ingredients are excluded.

Synthetic datasets were created in two steps.
%
First, unconditional preferences for individual ingredients were generated based on predefined user profiles.
%
Each profile specifies ranges of values for ingredients or categories (e.g., vegetables, meat), which are used to compute likelihoods via uniform distribution.
%
Second, recipe likability labels (like/dislike) were generated based on the likelihood values of the recipe's ingredients.
%
The synthesis process ensures no real personal data is used, avoiding ethical or privacy concerns.


\subsubsection{Learning User Preferences via Sub-Symbolic Predictors}\label{subsubsec:learning-user-preferences}
%
User preferences were modeled using fully-connected neural networks.
%
Each network consists of one input layer, two hidden layers, and one output layer, with 1032, 16, 8, and 1 neurons, respectively.
%
The activation functions for the input and hidden layers are \gls{ReLU}, while the output layer uses a sigmoid function.
%
The input to the network is a tensor representing the presence of 1032 ingredients in a recipe, and the output is a scalar indicating the likelihood of user appreciation.

A separate neural network was trained for each of the 12 synthetic users.
%
Training was performed on half of the dataset (22,874 records), while the remaining half was used for testing.
%
The training process lasted 20 epochs with a batch size of 32.
%
The average accuracy achieved on the test set was 85.6\%, with precision computed as the ratio of true positives to total positive predictions.
%
Precision is critical for systems where identifying true positives (liked recipes) is more important than minimizing false positives.

\subsubsection{Extracting User Preferences via \Gls{SKE}}\label{subsubsec:extracting-user-preferences}
%
\input{tables/net-rules-accuracy}
%
Symbolic knowledge was extracted from the trained \gls{ML} predictors using the CART algorithm~\cite{DBLP:books/wa/BreimanFOS84}.
%
This algorithm generates decision trees, which are converted into logic rules.
%
Each path from the root to a leaf in the decision tree corresponds to a rule, where nodes represent logical conditions (e.g., presence or absence of ingredients) and leaves denote the predicted class.

The maximum number of leaves was set to \( R = 50 \), and the maximum depth of the decision tree was limited to \( D = 10 \).
%
These constraints balance computational efficiency and interpretability, ensuring the rules are concise and comprehensible.
%
For example, extracted rules for a user might include:
%
\begin{align*}
  \text{likes}(R) &\leftarrow \text{has\_no}(R, \text{egg}) \land \text{has\_no}(R, \text{pepper}) \land \text{has}(R, \text{almond}) \\
  \neg \text{likes}(R) &\leftarrow \text{has}(R, \text{egg}) \land \text{has}(R, \text{parsley})
\end{align*}
%
These rules approximate the decision-making process of the neural network, enabling explainability.

We constrain the output rules for two main reasons.
%
First, limiting the growth of the decision tree (\gls{DT}) reduces computational complexity.
%
Given \(N\) binary features, such as ingredients, the maximum depth of the \gls{DT} is \(N + 1\), and the maximum number of leaves is \(2^N\).
%
For \(N = 1032\), this would be computationally infeasible.
%
Second, we aim to ensure the extracted rules remain interpretable.
%
Rules with excessively long right-hand sides or an overwhelming number of conditions are difficult for humans to read and understand.
%
This trade-off prioritizes interpretability over performance metrics, such as accuracy and precision, which is essential for explaining why a prescription may or may not be suitable for a user.

\Cref{tab:net-rules-stats} summarizes the accuracy of the extracted rules on the test sets.
%
It also reports the fidelity of the rules with respect to the sub-symbolic predictors.
%
Fidelity is computed similarly to accuracy but compares the predictions of the extracted rules against the class values predicted by the \gls{ML} predictor, rather than the ground truth.
%
In other words, fidelity measures how closely the extracted rules replicate the behavior of the neural networks.


\subsubsection{Proposed Recipes}\label{subsubsec:proposed-recipes}
%
\input{tables/proposed-recipes-accuracy}
%
\input{tables/proposed-recipes-with-nn-accuracy}
%
The experiments aim to evaluate how user preferences and expert prescriptions are combined to recommend recipes.
%
We rely on sets of logic rules to express domain-expert prescriptions, ensuring consistency with the formalism used for user preferences.
%
In total, six prescriptions are defined, corresponding to three days with two meals per day.
%
For each meal, multiple rules (ranging from two to four) are specified, one for each dish.

\begin{prescriptions}
  \item First day, lunch: ``Rice with vegetables.'' Ingredients include 80 grams of raw rice, 35 grams of raw lentils, 120 grams of raw chicken, 120 grams of mixed vegetables, garlic, herbs, 2 teaspoons of olive oil, and 1 orange.
  %
  \begin{align*}
    \begin{array}{rcl}
    \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{chicken}) \land \text{has}(R, \text{rice}) \\
    \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{lentils}) \\
    \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{orange}) \\
    \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{garlic})\\
                          & \land & \text{has}(R, X) \land \text{has}(R, Y) \land \text{has}(R, Z)\\
                          & \land & \text{vegetable}(X) \land \text{herb}(Y) \land \text{essential\_oil}(Z)
    \end{array}
  \end{align*}
  %
  \item First day, dinner: ``Burger and grilled vegetables.'' Ingredients include 90 grams of beef, 80 grams of bread, 120 grams of vegetables, 1 teaspoon of oil, and 1 cup of strawberries.
  %
  \begin{align*}
    \begin{array}{rcl}
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{beef}) \land \text{has}(R, \text{bread}) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{strawberry}) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, X) \land \text{has}(R, Y) \\
                            &\land& \text{vegetable}(X) \land \text{essential\_oil}(Y)
    \end{array}
  \end{align*}
  %
  \item Second day, lunch: ``Tuna salad.'' Ingredients include 120 grams of tuna, 120 grams of vegetables, 1 teaspoon of olive oil, 80 grams of bread, 35 grams of raw beans, and 1 cup of blueberries.
  %
  \begin{align*}
    \begin{array}{rcl}
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{bread}) \land \text{has}(R, \text{beans}) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{tuna}) \\
                            &\land& \text{has}(R, X) \land \text{has}(R, Y) \\
                            &\land& \text{vegetable}(X) \land \text{herb}(X) \land \text{essential\_oil}(Y) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{blueberry})
    \end{array}
  \end{align*}
  %
  \item Second day, dinner: ``Chicken with mustard and lemon juice.'' Ingredients include 90 grams of chicken, 120 grams of vegetables, 80 grams of raw pasta, 1 teaspoon of olive oil, mustard, lemon juice, and 1 cup of clementines.
  %
  \begin{align*}
    \begin{array}{rcl}
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{chicken}) \land \text{has}(R, \text{mustard}) \land \text{has}(R, \text{lemon\_juice}) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{pasta}) \\
                            &\land& \text{has}(R, X) \land \text{essential\_oil}(X) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, X) \land \text{vegetable}(X) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{citrus\_fruits})
    \end{array}
  \end{align*}
  %
  \item Third day, lunch: ``Salmon with potatoes.'' Ingredients include 120 grams of salmon, 240 grams of cooked potatoes, 120 grams of vegetables, 1 teaspoon of butter, and 1 pear.
  %
  \begin{align*}
    \begin{array}{rcl}
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{compound\_salmon}) \land \text{has}(R, \text{potato}) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{pear}) \\
      \text{should\_eat}(R) &\leftarrow& \text{has}(R, \text{butter}) \\
                            &\land& \text{has}(R, X) \land \text{vegetable}(X)
    \end{array}
  \end{align*}
  %
  \item Third day, dinner: ``Turkey in papillote.'' Ingredients include 90 grams of turkey, 1 teaspoon of olive oil, 120 grams of vegetables, 35 grams of raw gram beans, 80 grams of raw wholegrain rice, and 1 orange.
  %
  \begin{align*}
    \begin{array}{rcl}
      \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{gram\_bean}, \\
      \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{rice}) \\
      \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{orange}) \\
      \text{should\_eat}(R) & \leftarrow & \text{has}(R, \text{turkey}) \\
                            & \land & \text{has}(R, X) \land \text{has}(R, Y) \\
                            & \land & \text{vegetable}(X) \land \text{essential\_oil}(Y)
    \end{array}
  \end{align*}
\end{prescriptions}
%
For each user and prescription, recipes are computed based on the logic rules.
%
Precision is calculated as the ratio of recipes liked by users to the total number of proposed recipes.
%
Corner cases, where no recipes are recommended due to conflicting preferences and prescriptions, are resolved by adjusting prescriptions to better align with user preferences.


\subsection{Results and Discussion}\label{subsec:results-and-discussion}
%
To ensure realistic experiments, we adopted criteria to generate synthetic datasets representing user preferences.
%
We avoided synthesizing users with trivial rules, such as always liking a specific ingredient, which would result in predictable recommendations.
%
To address this, we introduced noise into the dataset synthesis process, assigning preference values within distributions and stochastically labeling classes.
%
This approach discourages oversimplified logic rules and mimics real-life scenarios where diverse ingredient combinations influence user preferences in complex ways.
%
\Cref{tab:net-rules-stats} reports the accuracy of the neural networks trained to predict user preferences, alongside the accuracy and fidelity of the extracted logic rules.
%
The accuracy of individual networks ranges from \(0.74\) to \(0.97\), with a mean value of approximately \(0.86\).
%
This variability reflects differences in user profiles, as some preferences are inherently more predictable than others.
%
The accuracy of the extracted rules ranges from \(0.68\) to \(0.86\), with a mean value of \(0.76\).
%
The mean difference of \(0.095\) between network accuracy and rule accuracy is expected, given the constraints imposed on decision tree depth during the extraction process.
%
Similar observations apply to precision measures.
%
It is important to note that extracted rules approximate the decision-making process of the neural networks and cannot outperform the original models.

\Cref{tab:proposed-recipes-stats-nn} summarizes the precision obtained during the recommendation phase using prescriptions and extracted rules.
%
The mean precision value across all experiments is approximately \(0.74\), which is close to the average precision of the rules reported in \Cref{tab:net-rules-stats}.
%
Applying the Student's \(t\)-test to the precision values in \Cref{tab:net-rules-stats} (``r. prec.'' column) and \Cref{tab:proposed-recipes-stats} (``average'' column) yields a \(p\)-value of \(0.386\).
%
This indicates no statistical difference between the two distributions.
%
In other words, recommending recipes liked by users from the entire dataset is as effective as recommending recipes prescribed by human experts.

\Cref{tab:proposed-recipes-stats-nn} compares the precision obtained using prescriptions and sub-symbolic predictors.
%
Similar statistical analysis yields a \(p\)-value of \(0.403\), leading to the same conclusion: the recommendation process is equally effective for prescribed liked recipes and liked recipes predicted by neural networks.

In summary, experiments demonstrate that sub-symbolic predictors outperform symbolic predictors in terms of overall precision for personalized food recommendations.
%
However, the primary goal of the framework is not to achieve higher performance compared to sub-symbolic predictors or existing systems.
%
Instead, the focus is on enhancing explainability, enabling users and experts to understand why certain recipes are recommended or rejected.
%
Extracted rules provide insights that allow experts to adjust prescriptions to better align with user preferences.
%
Despite lower performance compared to neural networks, the rules remain acceptable in real-world scenarios.
%
For instance, consider the recipe ``Shakkara (Sweet) Pongal'' (recipe ID \(4,055\)), which is liked by User~1.
%
This recipe contains ingredients such as basmati rice, butter, camphor, cardamom, cashew nuts, lentils, milk, raisins, and sugar.
%
The recommendation is justified by the presence of milk and sugar, as indicated by the extracted rules for User~1.
%
Conversely, the recipe ``Lasagna Spinach Roll-Ups'' (recipe ID \(10,815\)) is disliked due to the presence of eggs and pepper.
%
The symbolic approach adds value by providing explainability, allowing motivations for recommendations to be derived systematically.



\section{A general-purpose protocol for multi-agent based explanations}\label{sec:a-general-purpose-protocol-for-multi-agent-based-explanations}
%
In section we present the work ``A General-Purpose Protocol for Multi-agent Based Explanations''~\cite{DBLP:conf/extraamas/CiattoMBAO23}, presented at the \gls{EXTRAAMAS} international workshop, 5th edition, 2023.
%
This contribution is quite different from the previous one, and in general from the other contributions in this thesis.
%
While the majority of the works presented in this thesis vertically focus on \gls{SKI} or \gls{SKE} methods, or on the design of systems that leverage such methods, this work is more horizontal, in the sense that it proposes a general-purpose protocol for multi-agent based explanations.
%
Nonetheless, this work still falls within the scope of this chapter because its contribution is relevant to the design of explainable \glspl{MAS} that can potentially leverage \gls{SKE} methods.


\subsection{Motivation}\label{subsec:introduction-general-purpose-protocol-for-multi-agent-based-explanations}
%
The current focus of \gls{XAI} research is on developing techniques to ``open up'' black-box models and provide insights into how an intelligent system reaches specific decisions or predictions~\cite{DBLP:journals/csur/GuidottiMRTGP19}.
%
These techniques include methods for visualizing the internal workings of the system, such as feature importance scores, attention maps, and decision trees.
%
The primary goal of these methods is to assist \gls{AI} experts in understanding the system's behavior, rather than addressing the needs of non-expert users who seek to understand \emph{why} the system behaves in a particular way.

However, the expectations of the \gls{XAI} community extend beyond merely interpreting black-box models.
%
Ideally, \gls{XAI} systems should autonomously provide explanations that go beyond describing how the system works~\cite{DBLP:conf/aiia/CiattoCOC19}.
%
These explanations should offer insights into \emph{why} the system behaves -- or fails to behave -- in a specific manner, potentially through autonomous interaction with the explainee.

To achieve this, recent research emphasizes the automation and interactivity of the explanation process~\cite{DBLP:conf/atal/CiattoSOC20}.
%
This involves designing \gls{AI} systems capable of generating explanations dynamically and tailoring them to the explainee's knowledge level and needs.
%
In this context, \glspl{MAS} emerge as a suitable paradigm for building intelligent explainable systems, as they inherently support interaction and autonomy.
%
Explanations can thus be modeled as multi-agent interactions, where the explainee and the explainer agent (either human or software) collaborate to achieve the shared goal of providing clear and effective explanations.

This work addresses the general problem of enabling interaction between explainee and explainer agents.
%
To this end, it proposes a general-purpose protocol for multi-agent-based recommendations and explanations.
%
The protocol defines the roles and responsibilities of the explainee and explainer agents, as well as the types of information exchanged to ensure clarity and effectiveness.
%
Notably, the protocol builds upon prior efforts to model explanations as multi-agent interactions~\cite{buzcu-prima-2022}.
%
Its key features include:
%
\begin{inlinelist}
    \item the separation of recommendations from explanations, and
    \item support for contrastive explanations.
\end{inlinelist}

As a secondary contribution, the paper introduces a \texttt{Spade}-based Python library, \texttt{PyXMas}, which implements the proposed protocol.
%
This library allows the integration of various explanation strategies and representations.
%
It serves as a foundation for developing intelligent explainable systems where recommendation and explanation behaviors are delegated to individual agents.
%
Overall, this contribution represents a significant step toward building \gls{XAI} systems capable of providing automatic and interactive explanations.


\subsection{Background}\label{subsec:background-general-purpose-protocol-for-multi-agent-based-explanations}
%
\paragraph{Interactive recommendation systems}
%
Interactive \glspl{RS} have gained significant attention due to their ability to dynamically provide personalised recommendations based on user feedback and interactions~\cite{knijnenburg-2010}.
%
The key aspect of interactivity lies in collecting user feedback during the recommendation session to refine subsequent recommendations.
%
For instance, some systems employ a one-shot recommendation approach, where questions learned offline from past interactions are posed to users before generating recommendations~\cite{christakopoulou-2016}.
%
The answers to these questions enable the system to personalise and improve future suggestions.

\Gls{XAI} has been increasingly integrated into \glspl{RS} to enhance transparency~\cite{buzcu-prima-2022,zhang-2020}.
%
This is achieved through iterative recommendation sessions, where the system not only provides recommendations but also explanations, leveraging the positive impact of transparency on user trust~\cite{odonovan-2008}.
%
For example, visual explanations, such as grouped bar charts, have been used to compare user preferences with recommendation attributes~\cite{millecamp-2019}.
%
Similarly, conversational explanations have been employed to mimic human salesmanship, persuading users to consider alternative options~\cite{shimazu-2002}.


\paragraph{Prior work on explanation protocols}
%
This work builds upon the protocol introduced in~\cite{buzcu-prima-2022}, which focuses on food recommendations and explanations.
%
In this protocol, users specify constraints, such as allergies, preferred or disliked ingredients, and desired cuisine types.
%
The system responds with a recipe suggestion and an explanation.
%
Users can accept, reject, or provide feedback on the recommendation or explanation, initiating a turn-based interaction until the session concludes.
%
This framework promotes transparency by combining recommendations and explanations, which can increase user acceptance.
%
However, unrequested explanations may impose cognitive load, highlighting the importance of parsimony~\cite{MuallaTKNCAGN22}.
%
Parsimonious explanations are defined as the simplest descriptions that adequately convey the situation.

To address this, the revised protocol proposed in this work allows users to request explanations dynamically.
%
It also supports ``zooming'' explanations, where additional details are provided only upon user request.
%
This approach enables users to control the level of detail, ensuring explanations are tailored to their needs.


\paragraph{\textsc{Spade}: multi-agent programming in python}
%
\textsc{Spade} is an open-source \gls{MAS} platform implemented in Python\footnote{\url{https://spade-mas.readthedocs.io}}.
%
It provides a modular and extensible library for developing intelligent agents capable of interacting with each other and their environment.
%
\textsc{Spade} systems are distributed, with agents operating on the same or different network nodes.
%
Agent activities are governed by concurrent behaviours, implemented as Python classes that developers can extend.

Unlike \textsc{Jade}~\cite{jadebook-2007}, which is Java-based, \textsc{Spade} leverages Python's ecosystem, facilitating integration with \gls{ML} and \gls{AI} frameworks.
%
Agent communication in \textsc{Spade} is mediated by the \gls{XMPP} protocol, ensuring robust, interoperable, and scalable interactions.
%
This design supports blended applications where agents interact with both humans and software agents.

\textsc{Spade} includes features such as communication protocols, message passing, and event handling.
%
It also supports the implementation of interaction protocols using finite-state machine behaviours.
%
These capabilities make \textsc{Spade} a powerful tool for developing intelligent agents, widely adopted in \gls{MAS} research and development.


\subsection{Explanation-based recommendation protocol}
\label{subsec:explanation-based-recommendation-protocol}
%
The term ``explanation'' originates from the Latin word \emph{explicare}, meaning ``to unfold.''
%
In this context, an explanation is understood as the process of clarifying the meaning of a concept.
%
This process is inherently interactive, involving an explainee and an explainer.
%
Explanations are thus considered a social protocol.

In human interactions, explanation protocols are typically informal and unstructured.
%
They involve an explainee seeking clarification from an explainer, who is assumed to possess greater knowledge.
%
Explainers adapt their strategies and level of detail to the explainee's needs and knowledge as the interaction progresses.
%
Explanations are generally provided upon request and may respond to prior information shared by either party.

Modern intelligent systems, such as \gls{XAI} systems, aim to support decision-making by providing recommendations.
%
In these systems, the user typically acts as the explainee, while the software system assumes the roles of both the recommender and the explainer.
%
By adopting a \gls{MAS} perspective, recommendation and explanation can be modeled as a single interaction protocol between two agents.
%
One agent, often the explainee, is a human user, while the other, the explainer, is a software agent.

The proposed protocol assumes that the user initiates the interaction by submitting a query.
%
Upon receiving the query, the agent generates a recommendation.
%
This recommendation is computed using available information, such as the user's profile, interaction history, and possibly aggregated data from other users.
%
The agent may utilize both symbolic reasoning and \gls{ML} predictors to generate the recommendation.

%
After receiving the recommendation, the user may either accept or reject it, or request an explanation.
%
The explanation phase may involve multiple rounds of interaction, where the user can ask for additional details or comparisons.
%
The agent provides the requested information, aiming to clarify the recommendation.
%
Ultimately, the user decides to accept or reject the recommendation based on the explanation provided.

Feedback from the user, including the acceptance or rejection of recommendations and the amount of explanatory information required, is used to improve future interactions.
%
In cases of rejection, the agent may also seek the reason for the rejection to refine its recommendation and explanation strategies.


Explanations in this protocol are always:
%
\begin{inlinelist}
    \item provided upon request,
    \item related to the recommendation, and
    \item tailored to the user.
\end{inlinelist}
%
They can be categorized into two types:
%
\begin{itemize}
    \item \textbf{Ordinary explanations:} These address the question, ``Why did you recommend this?''
    %
    \item \textbf{Contrastive explanations:} These address the question, ``Why did you not recommend that instead?''
\end{itemize}
%
The protocol supports both types of explanations, allowing the user to choose the type they prefer.
%
The content and structure of the exchanged messages depend on the type of explanation requested.


\subsection{Abstract Formulation of the Protocol}
\label{subsec:abstract-formulation-of-the-protocol}
%
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/pyxmas/user-agent-protocol}
    \caption[Message communication diagram between an explainer agent and an explainee]{%
      Message communication diagram between an explainer agent (blue boxes) and an explainee (green boxes).
      %
      Each box represents a message.
      %
      Arrows indicate the possible replies to each message.
    }
    \label{fig:protocol-messages}
\end{figure}
%
\Cref{fig:protocol-messages} presents an abstract formulation of the proposed protocol, which is independent of the specific representation or computation of recommendations and explanations.
%
The focus is on the exchange of messages between the explainer and the explainee, the information these messages carry, and the sequence in which they are exchanged.
%
The protocol defines two roles:
%
\begin{itemize}
    \item The explainee, who initiates the interaction.
    %
    \item The explainer, who responds to the explainee's queries.
\end{itemize}
%
Five data types are identified as the potential payloads exchanged during the protocol:
%
\begin{itemize}
    \item \textbf{Queries} (\(Q\)): Requests for recommendations submitted by the explainee.
    %
    \item \textbf{Recommendations} (\(R, R'\)): Responses to queries, provided by the explainer.
    %
    \item \textbf{Explanations} (\(E, E'\)): Information provided by the explainer to clarify recommendations.
    %
    \item \textbf{Features} (\(F\)): User-specific aspects that justify the rejection of a recommendation, which the explainer should consider in future interactions.
    %
    \item \textbf{Motivations} (\(M\)): Reasons for rejecting a recommendation, which may influence the explainer's behavior.
\end{itemize}
%
Thirteen message types are defined, each represented as a named record of the form \texttt{Name(Payload)}, where \texttt{Payload} consists of instances of the aforementioned data types.
%
Optional fields in the payload are denoted with a question mark.
%
The message types are summarized as follows:
%
\begin{enumerate}
    \item \texttt{Query(\(Q\))}: Sent by the explainee to initiate the protocol, carrying a recommendation request.
    %
    \item \texttt{Recommendation(\(Q, R\))}: Sent by the explainer in response to a query, carrying the query and the computed recommendation.
    %
    \item \texttt{Why(\(Q, R\))}: Sent by the explainee to request an explanation for a recommendation.
    %
    \item \texttt{WhyNot(\(Q, R, R'\))}: Sent by the explainee to request a contrastive explanation, comparing the recommendation with an alternative.
    %
    \item \texttt{Accept(\(Q, R, E?\))}: Sent by the explainee to accept a recommendation, optionally including the explanation.
    %
    \item \texttt{Collision(\(Q, R, F, E?\))}: Sent by the explainee to notify the explainer of a conflict between the recommendation and a personal feature.
    %
    \item \texttt{Disapprove(\(Q, R, M, E?\))}: Sent by the explainee to reject a recommendation, providing a reason and optionally the explanation.
    %
    \item \texttt{Details(\(Q, R, E\))}: Sent by the explainer to provide additional details about a recommendation.
    %
    \item \texttt{Comparison(\(Q, R, R', E\))}: Sent by the explainer to provide a contrastive explanation when the alternative recommendation is also valid.
    %
    \item \texttt{Invalid(\(Q, R', E\))}: Sent by the explainer to indicate that the alternative recommendation is invalid, with an explanation.
    %
    \item \texttt{Unclear(\(Q, R, E\))}: Sent by the explainee to indicate that the provided explanation is unclear.
    %
    \item \texttt{Prefer(\(Q, R, R'\))}: Sent by the explainee to express a preference for an alternative recommendation.
    %
    \item \texttt{Override(\(Q, R, R'\))}: Sent by the explainee to enforce an alternative recommendation, even if deemed invalid by the explainer.
\end{enumerate}
%
This abstract formulation ensures flexibility, allowing implementers to tailor the protocol to specific application domains.

Notably, messages are designed by keeping the \gls{ReST} [9] architectural style into account.
%
Hence, each message type is designed to carry all the information necessary for any involved party to decide which action to take next.
%
This is the reason why all/most messages carry the original query $Q$ and the recommendation $R$ (or $R'$) which they are referring to.
%
The message communication diagram from \Cref{fig:protocol-messages} depicts not only the messages exchanged by the explainee and explainer, but also the admissbile request-response patterns which the protocol allows.
%
There, a more detailed view of the message flow is provided, which we briefly summarise in the following.


\subsubsection{Relevant scenarios and protocol analysis}\label{subsubsec:relevant-scenarios}
%
\input{figures/protocol-sequence-diagrams}
%
The proposed protocol is versatile and accommodates various user needs and desires.
%
These include:
%
\begin{inlinelist}
    \item requesting a recommendation,
    \item seeking an explanation for the recommendation,
    \item asking for additional details about the explanation,
    \item simulating alternative recommendations, and
    \item providing feedback on recommendations or explanations.
\end{inlinelist}
%
\Cref{fig:protocol-sequence-diagrams} illustrates these scenarios, which are detailed below.

%
\paragraph{Quick Accept}
%
In this scenario, depicted in \Cref{fig:quick-accept}, the user accepts the recommendation without requiring an explanation.
%
For example, a user requests a restaurant recommendation, and the agent suggests a restaurant that the user finds satisfactory.

%
\paragraph{Quick Retry}
%
In \Cref{fig:quick-retry} the user rejects the recommendation without asking for an explanation.
%
The rejection may occur because the recommendation conflicts with the user's preferences or is unsuitable for the current context.
%
For instance, if the agent recommends a steakhouse to a vegetarian user, the user may signal a conflict with their preferences.
%
Alternatively, the user may simply disapprove of the recommendation without providing specific feedback.
%
In both cases, the agent generates a new recommendation.
%
The agent is expected to learn from conflicts but not from generic disapprovals.

%
\paragraph{Ordinary Explanation Loop}
%
In the case shown in \Cref{fig:explanation-loop}, the user requests an explanation for the recommendation.
%
If the explanation is unsatisfactory, the user may ask for further details, initiating an iterative process.
%
This loop continues until the user either accepts the recommendation or requests a new one.
%
The protocol supports ``zooming'' explanations, where the agent adjusts the granularity of the explanation.
%
For example, the agent may first provide local explanations, describing how the specific recommendation was generated.
%
Subsequently, it may offer global explanations, detailing the general logic behind its recommendations.
%
The agent can also switch between textual and visual explanations to enhance clarity.
%
Consider a user who requests a restaurant recommendation.
%
The agent suggests an Asian restaurant with a high rating and proximity to the user.
%
If the user asks for an explanation, the agent may state that the restaurant matches the user's taste for sushi and is within 1 km.
%
If further details are requested, the agent may explain its general recommendation strategy, such as prioritizing highly rated restaurants within a certain distance.

%
\paragraph{Contrastive Explanation Loop}
%
Lastly, in \Cref{fig:contrastive-explanation-loop}, the user requests a contrastive explanation, comparing the given recommendation \(R\) with an alternative \(R'\).
%
If \(R'\) is valid, the agent provides a comparison, highlighting why one recommendation is preferable.
%
If \(R'\) is invalid, the agent explains why it cannot be recommended.
%
The user may then accept the original recommendation, prefer the alternative, or override the agent's decision.
%
For example, if the agent recommends an Asian restaurant, but the user prefers a steakhouse, the agent may compare the two options.
%
If the steakhouse aligns with the user's dietary goals, the agent may note that the Asian restaurant is closer.
%
If the steakhouse violates dietary goals, the agent explains this conflict.
%
In either case, the user decides whether to accept the original recommendation or override it.
%
The agent learns from overrides to refine future recommendations.


\subsubsection{Which Sorts of Explanations and Recommendations?}
\label{subsubsec:which-sorts-of-explanations-and-recommendations}

The proposed explanation protocol is agnostic regarding the specific representation of explanations and recommendations.
%
It is the responsibility of the implementer to define how explanations and recommendations are represented and computed.
%
The protocol only specifies \emph{when} these elements should be computed.

\Glspl{RS} typically rely on one or more \gls{ML} predictors trained on user data.
%
Whether the training of these predictors is performed by the recommender agent or if the agent is equipped with pre-trained predictors at deployment is an implementation detail.
%
In either case, the recommender agent must have access to user profile information.
%
This information can be obtained during an initial configuration phase or inferred from user interactions, such as accepted or rejected recommendations.
%
To support dynamic learning, the agent should include a learning algorithm capable of updating the predictors when new user data becomes available.
%
From this perspective, the explainer agent acts as a proxy for the \gls{ML} predictor(s).

Explanations, however, are not necessarily derived from \gls{ML} predictors.
%
The \gls{XAI} literature offers a wide range of approaches for generating explanations, including visual, textual, and numerical methods~\cite{DBLP:journals/inffus/ArrietaRSBTBGGM20,DBLP:journals/csur/GuidottiMRTGP19,DBLP:journals/csur/CiattoSAMO24}.
%
The explainer agent must not only wrap the \gls{ML} predictor(s) but also encapsulate the logic for computing and representing explanations.

A critical challenge arises when recommendations and explanations use different representation formats, such as textual and visual.
%
In such cases, the explainer agent must bridge the gap by providing a unified representation of both the recommendation and its explanation.
%
To address this, designers may consider adopting computational logic as a unifying framework for recommendations and explanations.
%
In computational logic, both knowledge bases and queries are represented as logic formulas.
%
These formulas can be used to represent recommendation requests, solutions, and explanations.

For example, a recommendation query can be expressed as the logic goal:
%
\[
\texttt{should\_eat(Food, lunch)},
\]
%
where \texttt{Food} is a logic variable representing an unknown value.
%
Recommendations, such as \( R, R', R'' \), correspond to logic solutions, e.g., \(\texttt{Food} = \texttt{paella}\).
%
Explanations \( E, E', E'' \) can take various forms:
%
\begin{itemize}
    \item \textbf{Local explanations:} The path in the proof tree computed by the explainer agent to derive the recommendation.
    %
    \item \textbf{Global explanations:} The logic program used by the explainer agent to generate the recommendation.
    %
    \item \textbf{Contrastive explanations:} Metrics comparing multiple recommendations or identifying constraints that make certain recommendations invalid.
    %
    \item \textbf{Combinations:} Any combination of the above types.
\end{itemize}

User features, such as \( F, F', F'' \), may include raw facts describing the user, e.g., \texttt{age(31)}, \texttt{goal(lose\_weight)}, or \texttt{category(vegetarian)}.
%
Motivations for disapproval can include predefined facts, such as:
%
\begin{itemize}
    \item \texttt{dislike:} The user dislikes the recommendation, prompting the agent to learn from this feedback.
    %
    \item \texttt{not\_now:} The user does not want the recommendation at the moment, but may accept it in the future.
    %
    In this case, the agent should not memorize the rejection.
    %
\end{itemize}


\subsection{From Theory to Practice with \textsc{PyXMas}}
\label{subsec:from-theory-to-practice-with-pyxmas}
%
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/pyxmas/architecture}
    \caption[Modular architecture of \textsc{PyXMas}]{%
        Modular architecture of \textsc{PyXMas}.
        %
        The library provides parametric behaviours that can be customized by implementing specific components.
        %
        If the explainee agent is human, a UX component may be required to facilitate interaction via a device.
    }
    \label{fig:architecture}
\end{figure}
%
This section describes how the proposed protocol can be implemented as agent-oriented software.
%
We introduce \textsc{PyXMas}\footnote{\url{https://github.com/pikalab-unibo/pyxmas}}, a Python library for explainable multi-agent systems (\glspl{MAS}).
%
The library is built on \textsc{Spade} and provides reusable implementations of the protocol described in \Cref{subsec:abstract-formulation-of-the-protocol}.
%
This allows researchers and developers to focus on designing recommender and explainer agents, as well as defining the representation of recommendations and explanations, without re-implementing the protocol.


\subsubsection{\textsc{PyXMas} Architecture}
\label{subsubsec:pyxmas-architecture}
%
\Cref{fig:architecture} illustrates the modular architecture of \textsc{PyXMas}.
%
The library defines two main behaviours:
%
\begin{inlinelist}
    \item the \emph{initiator}, responsible for sending recommendation queries and processing responses, and
    %
    \item the \emph{responder}, responsible for computing and returning recommendations and explanations.
\end{inlinelist}
%
The initiator behaviour is designed for the explainee agent, while the responder behaviour is intended for the explainer agent.
%
Both behaviours are parametric, allowing users to customize their functionality by implementing specific components.

\paragraph{Explainer Agent}
%
The explainer agent requires the following components:
%
\begin{itemize}
    \item \textbf{Recommendation Strategy:} Computes recommendations based on user queries, preferences, and goals (e.g., ``vegetarian'' or ``weight loss'').
    %
    The strategy can adapt over time by learning from user feedback.
    %
    \item \textbf{Explanation Strategy:} Generates explanations for recommendations, leveraging user profiles and domain knowledge (e.g., ``ingredient X is plant-based'').
    %
    \item \textbf{User Profiler:} Learns user preferences from feedback using heuristic or \gls{ML}-based methods.
    %
    This enables the agent to refine recommendations and explanations dynamically.
    %
    \item \textbf{Interaction Strategy:} Manages how recommendations and explanations are presented to the explainee.
    %
    For example, a humanoid robot may use gestures or facial expressions to enhance interaction.
\end{itemize}
%
The explainer agent stores two types of data:
%
\begin{inlinelist}
    \item user profile data, and
    %
    \item domain knowledge.
\end{inlinelist}
%
These are maintained in dedicated data stores and updated as needed.

\paragraph{Explainee Agent}
%
The explainee agent requires the following components:
%
\begin{itemize}
    \item \textbf{Query Provider:} Generates queries based on the explainee's goals.
    %
    \item \textbf{Recommendation Evaluator:} Assesses recommendations and decides whether to accept or reject them.
    %
    \item \textbf{Explanation Evaluator:} Evaluates explanations and influences the recommendation evaluator accordingly.
\end{itemize}
%
If the explainee is a human user, the agent acts as a proxy, mediating interactions via a \gls{UI} on a device (e.g., a smartphone).
%
In this case, a \textbf{\gls{UX}} component is required to manage the \gls{UI}, process user inputs, and present recommendations and explanations.


\subsubsection{PyXMas Design}\label{subsubsec:pyxmas-design}
%
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/pyxmas/types}
    \caption{
        Abstract classes for message payloads in \textsc{PyXMas}.
    }
    \label{fig:types}
\end{figure}
%
\textsc{PyXMas} is a Python library designed for developing explainable \glspl{MAS}.
%
It provides:
%
\begin{itemize}
    \item Abstract classes for the (de)serialization of message payloads exchanged between the explainer and explainee agents (see \Cref{fig:types}).
    %
    \item Abstract classes defining the initiator and responder behaviors.
\end{itemize}
%
These abstract classes allow developers to extend and customize their functionality by overriding specific methods.
%
This flexibility enables the creation of tailored explainable \glspl{MAS}.
%
\paragraph{Data Types for Message Payloads}
%
As illustrated in \Cref{fig:types}, \textsc{PyXMas} provides five abstract classes corresponding to the data types defined in \Cref{subsec:abstract-formulation-of-the-protocol}.
%
These classes enforce serialization, ensuring that data can be converted to and from strings.
%
This is essential for enabling communication between the explainer and explainee agents over the network.
%
Developers can define custom representations for queries, recommendations, explanations, and other data types by extending these abstract classes.
%
The only requirement is that the serialized data must be both machine- and human-readable.
%
\paragraph{Predefined Behaviors}
%
\begin{figure}
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pyxmas/user-state-diagram}
        \caption{
            Initiator-side state diagram.
        }
        \label{fig:user-state-diagram}
    \end{subfigure}
    %
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pyxmas/agent-state-diagram}
        \caption{
            Responder-side state diagram.
        }
        \label{fig:agent-state-diagram}
    \end{subfigure}
    \caption{
        State diagrams describing the initiator and responder behaviors in \textsc{PyXMas}.
    }
    \label{fig:state-diagrams}
\end{figure}
%
\textsc{PyXMas} provides two abstract classes for the protocol roles:
%
\begin{itemize}
    \item The \emph{initiator}, which represents the explainee agent.
    %
    \item The \emph{responder}, which represents the explainer agent.
\end{itemize}
%
These behaviors are implemented as finite-state machines (FSMs) using \textsc{Spade}.
%
Each state corresponds to a specific action, such as waiting for a message or processing a response.
%
\Cref{fig:state-diagrams} illustrates the state diagrams for the initiator and responder behaviors.
%
On the initiator side, developers can override callbacks to control:
%
\begin{itemize}
    \item Query generation.
    %
    \item Evaluation of recommendations and decisions to accept or reject them.
    %
    \item Evaluation of explanations and their influence on recommendation acceptance.
\end{itemize}
%
On the responder side, developers can override callbacks to control:
%
\begin{itemize}
    \item Recommendation generation.
    %
    \item Explanation generation.
    %
    \item Handling of accepted or rejected recommendations.
\end{itemize}
%
This design ensures that \textsc{PyXMas} can be adapted to various application domains while maintaining a clear and modular structure.


\section[\Gls{NeSy} \Gls{AI} for disease diagnosis and monitoring]{\Gls{NeSy} \gls{AI} for supporting Chronic disease diagnosis and monitoring}
\label{sec:nesy-ai-for-supporting-chronic-disease-diagnosis-and-monitoring}
%
\subsection{Motivations and background}
\label{subsec:motivations-and-background}
%
Telemedicine platforms are increasingly used for personal assistance, enabling patients to share their symptoms, medical history, and diagnostic data, such as medical images, lab results, or vital signs.
%
These platforms often leverage \gls{AI} data-driven models~\cite{HunterNEJMra2212850,topol2022}.
%
While promising, accurate disease monitoring and diagnosis require more than identifying patterns in data.
%
The integration of \gls{AI} into clinical practice remains limited due to several challenges.
%
First, the performance of \gls{AI} models is often suboptimal, primarily due to insufficient dataset sizes for effectively training \gls{ML} algorithms~\cite{benjamens2020state}.
%
Second, even high-performing \gls{AI} models frequently operate as black boxes, making their outputs difficult to interpret.
%
This lack of transparency hinders adoption in clinical settings, where healthcare professionals require interpretable outputs to make informed decisions~\cite{ALI2023107555}.
%
Traditional clinical protocols, while validated and robust, also exhibit limitations.
%
They often fail to classify all clinical cases, leaving certain scenarios unresolved.
%
Additionally, these protocols do not leverage data analytics, which could uncover hidden patterns and provide valuable insights for decision-making.

Combining data-driven and knowledge-based models has been identified as a promising solution to enhance \gls{ML} performance and improve explainability~\cite{von2021informed}.
%
This work focuses on \gls{NeSy} \gls{AI}, specifically \gls{SKI}~\cite{DBLP:journals/csur/CiattoSAMO24}.
%
Among the various \gls{SKI} methods, we evaluate the \gls{LTN} approach~\cite{BadreddineGSS22}.
%
The potential of this approach is demonstrated using the \gls{PID} dataset for diabetes prediction~\cite{smith1988using}.
%
We compare its performance against classical \gls{ML} models, including \gls{KNN}, \gls{LR}, \gls{DT}, \gls{RF}, and an uneducated \gls{MLP}.
%
Performance is evaluated based on criteria such as classification accuracy and robustness.

Results indicate that classical \gls{ML} models struggle to achieve consistent performance.
%
In contrast, injecting domain knowledge into neural networks via the \gls{LTN} method significantly improves clinically relevant metrics, such as recall, balanced accuracy, F1 score, and Matthews Correlation Coefficient~\cite{MATTHEWS1975442}.
%
Moreover, \gls{LTN} enhances adherence to clinical knowledge, a critical requirement in the medical domain.
%
It also demonstrates high robustness against data manipulation, including noise injection, data dropping, and label flipping.

\subsection{Knowledge integration in medicine}
\label{subsec:knowledge-integration-in-medicine}

The integration of medical knowledge into \gls{ML} pipelines can occur at various stages~\cite{von2021informed,kierner2023taxonomy}.
%
\begin{itemize}
    \item \textbf{Data pre-processing:} Errors in datasets can be mitigated by removing anomalous samples based on clinical norms.
    %
    Virtual samples adhering to medical knowledge can be generated to address insufficient or missing data.
    %
    \item \textbf{Feature engineering:} Novel features can be derived using mathematical or logical models informed by medical knowledge.
    %
    Feature selection can also be guided by prior knowledge.
    %
    \item \textbf{Model learning:} Rules can be incorporated into the model's loss function or architecture.
    %
    \item \textbf{Output evaluation:} \gls{ML} outputs can be integrated with clinical guidelines to filter predictions or verify consistency with domain knowledge.
\end{itemize}
%
While these strategies are promising, they often do not reference the \gls{SKI} framework or its complementary \gls{SKE} paradigm~\cite{DBLP:journals/csur/CiattoSAMO24}.
%
These paradigms have proven effective in other domains and hold potential for healthcare applications.
%
Preliminary experiments~\cite{sirocchiRuleML2024} suggest that further evaluation of \gls{SKI} methods, such as \gls{LTN}, is warranted.


\subsection{Materials and methods}
\label{subsec:materials-and-methods}
%
In this study, we utilize the \gls{PID} dataset~\footnote{\url{https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database}}, a widely recognized benchmark in medical-related \gls{ML} research~\cite{karegowda2011application,DBLP:journals/nca/ChangBXS23}.
%
We focus on the \gls{LTN} approach~\cite{BadreddineGSS22}, a \gls{SKI} method that combines structuring and constraining injection strategies.
%
This section details the dataset, the \gls{LTN} methodology, and the metrics used to evaluate the impact of \gls{SKI} on predictive performance and \gls{QoS} aspects.

%
\subsubsection{Dataset and knowledge}
\label{subsubsec:dataset-and-knowledge}
%
The \gls{PID} dataset consists of 768 medical profiles of women aged 21 and above who underwent an oral glucose tolerance test.
%
The dataset includes features such as glucose and insulin concentrations, blood pressure (BP), skin thickness (ST), body mass index (BMI), diabetes pedigree function, number of pregnancies, and age.
%
The target variable is binary, indicating whether diabetes was diagnosed within five years.

%
Missing values are present in several attributes, including insulin (48.70\%), ST (29.56\%), BP (4.55\%), BMI (1.43\%), and glucose (0.65\%).
%
These missing values were imputed using the median value of the respective attribute.
%
The dataset is imbalanced, with 65.1\% of instances labeled as negative and 34.9\% as positive.

%
Public health guidelines on type-2 diabetes risks indicate that individuals with a high BMI (\(\geq 30\)) and high blood glucose levels (\(\geq 126\)) are at severe risk for diabetes.
%
Conversely, individuals with a normal BMI (\(\leq 25\)) and low blood glucose levels (\(\leq 100\)) are less likely to develop diabetes.
%
These guidelines are encoded into logic formulas as follows:
%
\begin{align}
    \forall x.\, (\text{glucose}(x) \geq 125 \land \text{bmi}(x) \geq 30 \rightarrow \text{Diabetic}(x)), \label{eq:diabetes_rule1} \\
    \forall x.\, (\text{glucose}(x) \leq 100 \land \text{bmi}(x) \leq 25 \rightarrow \neg \text{Diabetic}(x)). \label{eq:diabetes_rule2}
\end{align}

%
\subsubsection{Logic Tensor Network for \gls{SKI}}
\label{subsubsec:ltn-for-ski}
%
The \gls{LTN} framework~\cite{BadreddineGSS22} extends \gls{FOL} with fuzzy semantics, where formulas are interpreted as continuous truth values in the interval \([0, 1]\).
%
This allows the definition of logic rules that are not strictly true or false but partially true, with predicates computed by neural networks.

%
In this study, we use \gls{LTN} to inject the knowledge encoded in Equations~\eqref{eq:diabetes_rule1} and~\eqref{eq:diabetes_rule2}.
%
The predicate \(\text{Diabetic}(x)\) is modeled as a neural predicate backed by a feed-forward \gls{NN} with three hidden layers.
%
The NN outputs a real number in \([0, 1]\), representing the truth value of \(\text{Diabetic}(x)\) for an input \(x \in \mathbb{R}^8\).

%
The training process maximizes the truth value of the logic formulas while considering the available data.
%
This is achieved by minimizing a loss function that penalizes violations of the injected knowledge.
%
The predicates \(\text{glucose}(x)\) and \(\text{bmi}(x)\) are grounded to the actual values in the dataset, while the following dummy formulas are added to supervise the training:
%
\begin{align}
    \forall x.\, (\text{outcome}(x) = \text{diabetes} \rightarrow \text{Diabetic}(x)), \\
    \forall x.\, (\text{outcome}(x) = \text{healthy} \rightarrow \neg \text{Diabetic}(x)).
\end{align}

%
\subsubsection{Evaluation Metrics}
\label{subsubsec:evaluation-metrics}
%
To assess the impact of \gls{SKI}, we evaluate both predictive performance and \gls{QoS} metrics.

%
\paragraph{Performance Metrics}
%
Predictive performance is evaluated using standard metrics for binary classification, including accuracy, precision, recall, F1-score, balanced accuracy (BA), and the \gls{MCC}.

%
\paragraph{Logic Adherence}
%
Inspired by fidelity metrics in the \gls{SKE} framework~\cite{DBLP:journals/csur/CiattoSAMO24}, we propose a metric to assess adherence to clinical rules.
%
For each rule, we create a test set of instances satisfying the rule, assign labels based on the rule, and compute the model's accuracy on this subset.

%
\paragraph{Quality of Service}
%
\gls{QoS} metrics evaluate non-functional aspects such as robustness and complexity.
%
This study focuses on robustness to data degradation, which occurs when input data is noisy, incomplete, or biased.
%
Robustness is quantified using a reproducible procedure where training data is perturbed, and the model's performance is evaluated on the perturbed data~\cite{DBLP:journals/aamas/AgiolloRMCO23}.
%
The robustness score \(\rho_{N, D}(D)\) of a predictor \(N\) trained on \(D\) is defined in \Cref{eq:robustness-score}.
%
The robustness gain \(R_{N, D}(I)\) of an injection mechanism \(I\) is computed as shown in \Cref{eq:robustness-gain}.
%
A value \(R_{N, D}(I) > 1\) indicates improved robustness, while \(R_{N, D}(I) < 1\) indicates reduced robustness.


\subsubsection{Experimental Design}
\label{subsubsec:experimental-design}
%
The experiments are designed to evaluate the impact of the \gls{LTN} approach on the performance of \gls{ML} models in a clinical context.
%
The implementation of \gls{LTN} used in this study is publicly available\footnote{\url{https://github.com/pikalab-unibo/experiments-ltn-2024}}, along with the code for replicating the experiments.
%
The evaluation focuses on three main aspects:
%
\begin{enumerate}
    \item The predictive performance of \gls{ML} models, with particular attention to recall and F1-score, given the medical domain's sensitivity to false negatives.
    %
    \item The adherence of \gls{ML} models to established medical protocols, assessing whether the educated model aligns more closely with the injected knowledge compared to the uneducated one.
    %
    \item The robustness of \gls{ML} models to data degradation, analyzing whether the performance of the educated model degrades less than that of the uneducated model under perturbed data.
\end{enumerate}
%
To ensure a fair comparison, a grid search is performed to optimize the hyperparameters of all models.
%
The best hyperparameters for the baseline models are as follows:
%
\begin{itemize}
    \item \gls{KNN}: \(k = 7\), L1 distance.
    %
    \item \gls{DT}: Entropy as the split criterion, maximum depth of 10.
    %
    \item \gls{RF}: 50 trees, maximum depth of 20, entropy as the criterion, and a minimum of 5 samples per leaf.
    %
    \item \gls{LR}: Regularization parameter of 10, maximum iterations of 1,000, and \texttt{liblinear} as the solver.
\end{itemize}
%
The uneducated \gls{MLP} and the educated \gls{LTN} models share the same feedforward \gls{NN} architecture:
%
\begin{itemize}
    \item An input layer with 8 neurons, one for each feature in the dataset, using the \gls{LeakyReLU}~\cite{maas2013rectifier} activation function and batch normalization.
    %
    \item Three hidden layers with 256, 128, and 64 neurons, respectively, each using \gls{LeakyReLU} and batch normalization.
    %
    \item An output layer with a single neuron and a sigmoid activation function.
\end{itemize}
%
The training of the \gls{LTN} model consists of two phases.
%
First, the uneducated model is trained for 50 epochs using the Adam optimizer with a learning rate of \(10^{-4}\), weight decay of \(10^{-4}\), a batch size of 32, and a binary cross-entropy loss function.
%
An early stopping criterion is applied, with a patience of 5 epochs.
%
Second, the knowledge is injected, and the model is fine-tuned for 200 epochs using the same early stopping criterion but with a reduced learning rate of \(10^{-6}\).
%
All models are evaluated using multiple runs of a 50--50 train-test split, with performance metrics averaged over 50 runs.
%
Statistical significance is assessed using the Wilcoxon signed-rank test with a significance level of 0.05.
%
To evaluate robustness, the experiments include multiple runs with different types and magnitudes of data perturbations.
%
The perturbations include noise addition, sample drop, and label flipping, following the methodology in~\cite{DBLP:journals/aamas/AgiolloRMCO23}.
%
The noise magnitude \(\mu\) ranges from 0.1 to 1.0 in steps of 0.1, while the percentages of sample drop and label flipping vary from 0 to 0.9 in steps of 0.1.


\subsection{Results and Discussion}
\label{subsec:results-discussion}
%
We now present and discuss the results obtained for predictive performance metrics, logical adherence, and robustness.

%
\subsubsection{Performance Metrics}
\label{subsubsec:performance-metrics}
%
\input{tables/ski-diabetes-performance-metrics}
%
The performance metrics are summarized in \Cref{tab:ski-diabetes-performance-metrics}.
%
While \gls{LR} achieves the highest accuracy, this metric is less reliable for imbalanced datasets.
%
In this study, the minority class, representing diabetic instances, is the primary focus.
%
The \gls{LTN} model achieves the highest scores in balanced accuracy, F1-score, and \gls{MCC}.
%
These metrics are more suitable for evaluating clinical models as they account for class imbalance.
%
The \gls{LTN} also achieves the highest recall value of \(0.725\), which is critical in clinical settings.
%
Recall measures the proportion of diabetic patients correctly identified, minimizing false negatives.
%
False negatives can lead to missed diagnoses and untreated conditions, which are particularly problematic in healthcare.
%
In contrast, \gls{KNN} and \gls{DT} achieve recall values of \(0.583\) and \(0.580\), respectively, indicating their limited ability to detect positive cases.
%
\gls{LR} achieves a recall of \(0.560\), while \gls{RF} slightly improves with a recall of \(0.592\).
%
Although the \gls{MLP} shows some improvement with a recall of \(0.66\), the injection of domain knowledge via the \gls{LTN} yields a significant performance boost.
%
Precision, while highest for \gls{LR}, is less emphasized in clinical contexts, as false positives typically lead to follow-up testing with minimal consequences.
%
These results highlight the limitations of classical \gls{ML} models in achieving sufficient recall for clinical deployment.
%
The superior recall of the \gls{LTN} can be attributed to its incorporation of logic formulas, which effectively utilize background knowledge.
%
By grounding predictions in logical structures, the \gls{LTN} identifies patterns that simpler models may overlook.

%
\subsubsection{Logical Adherence}
\label{subsubsec:logical-adherence}
%
\input{tables/ski-diabetes-logic-adherence}
%
In clinical applications, models must not only make accurate predictions but also align with established medical protocols.
%
Adherence to clinical guidelines enhances trust and reliability, increasing the likelihood of adoption in practice.
%
The logical adherence results are presented in \Cref{tab:logic-adherence}.
%
All models, except for \gls{DT}, fully comply with the second rule, which classifies healthy patients.
%
However, the primary focus is on adherence to the first rule, which identifies diabetic patients.
%
The \gls{MLP} achieves \(82\%\) adherence to the first rule, outperforming classical \gls{ML} models such as \gls{RF}, which achieves \(76\%\).
%
The \gls{LTN} further improves adherence, achieving \(87\%\).
%
These results confirm that injecting domain knowledge through the \gls{LTN} modifies the model's structure, aligning it more closely with medical knowledge and clinical protocols.

%
\subsubsection{Robustness Analysis}
\label{subsubsec:robustness-analysis}
%
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ski-diabetes-robustness}
    \caption[Robustness analysis under noise injection, label flipping, and data dropping]{
        %
        Robustness analysis under noise injection, label flipping, and data dropping.
        %
        The \gls{LTN} demonstrates superior robustness across all metrics compared to other models.
    }
    \label{fig:robustness-analysis}
\end{figure}
%
Robustness refers to the model's ability to maintain performance under data perturbations, such as noise, missing data, or label flipping.
%
This evaluation is critical in clinical settings, where data collection is often noisy or incomplete.
%
The robustness results are illustrated in \Cref{fig:robustness-analysis}.
\note{TODO: generate again the figure because it is not vectorial and it has an error}
%
Under noise injection, the \gls{LTN} demonstrates the most stable performance across \gls{MCC}, balanced accuracy, F1-score, and recall.
%
Performance declines significantly only at noise levels above \(0.5\), which are uncommon in clinical environments.
%
For label flipping, the \gls{LTN} shows resilience up to a flipping rate of \(0.6\), with recall remaining virtually unchanged.
%
A notable drop is observed only at a flipping rate of \(0.7\), which is rare in practice.
%
In the case of data dropping, all models maintain stable performance up to a drop rate of \(0.8\).
%
However, the \gls{LTN} uniquely preserves recall, highlighting its ability to recover positive cases even with missing data.
%
These findings underscore the advantages of knowledge injection via the \gls{LTN}, which enhances robustness in complex clinical data scenarios.


%\subsection{\Gls{LLM}-based solutions for healthcare chatbots: a comparative analysis}\label{subsec:llm-based-solutions-for-healthcare-chatbots-a-comparative-analysis}

%\subsection{Open-source small language models for personal medical assistant chatbots}\label{subsec:open-source-small-language-models-for-personal-medical-assistant-chatbots}

\section[RAG on open LLM for medical chatbot]{Applying \glsentryshort{RAG} on open \glspl{LLM} for a medical chatbot supporting hypertensive patients}\label{sec:applying-rag-on-open-llm-for-a-medical-chatbot-supporting-hypertensive-patients}

\subsection{Motivations and Background}
\label{subsec:motivations-and-background-rag}
%
Managing chronic diseases, such as hypertension, poses significant challenges for both healthcare systems and patients.
%
These conditions require continuous monitoring, lifestyle adjustments, and often lead to substantial healthcare costs.
%
Frequent interactions with healthcare professionals can result in long wait times and limited accessibility for patients.
%
To address these issues, we propose the development of a chatbot to support patients in the self-management of chronic conditions, with a specific focus on hypertension.
%
The chatbot aims to empower hypertensive patients by providing timely, accurate, and empathetic guidance, particularly for acquiring vital signs and maintaining a healthy lifestyle~\cite{telmed2024,llm-goodit2023}.
%
Two critical requirements emerge for such a system.
%
First, the interaction must be empathetic to ensure patient engagement and motivation.
%
Second, the chatbot must provide highly accurate information, as no healthcare professional mediates the conversation.
%
Additionally, the system must comply with data privacy regulations, precluding the use of third-party systems for \gls{NLP} and \gls{NLG}.

%
Given these requirements, we consider \glspl{LLM} as the core technology for the chatbot.
%
\Glspl{LLM} have demonstrated the ability to generate trustworthy, reliable, and empathetic text, making them suitable for this application.
%
For instance, studies have shown that patients often prefer responses generated by \glspl{LLM} over those from physicians in online forums, rating the chatbot's quality and empathy higher~\cite{Chatbot-JAMAIntMed2023}.
%
To avoid reliance on proprietary third-party services, we focus on open \glspl{LLM}, including both domain-specific and general-purpose models.
%
To enhance their performance, we integrate \gls{RAG} techniques~\cite{Lewis-NIPS20}.
%
\Gls{RAG} enriches the \gls{LLM} with symbolic knowledge, which aligns with the definition of \gls{SKI} provided by~\cite{DBLP:journals/csur/CiattoSAMO24}.
%
Unlike traditional \gls{SKI} methods that rely on logic rules, \gls{RAG} incorporates symbolic knowledge in the form of free text, enabling the \gls{LLM} to generate contextually relevant and precise responses.

%
The \gls{RAG} approach involves constructing a knowledge base -- usually stored in a vectorial data base -- from data provided by medical professionals and enriching it with retrieval techniques.
%
This allows for a comprehensive comparison of retrieval strategies and \glspl{LLM}, both specialized and general-purpose.
%
Our findings demonstrate that \gls{RAG} significantly improves model performance, often surpassing specialized models in most tested cases.

%
\subsubsection{Applications of \Glspl{LLM} in Healthcare}
%
The adoption of \glspl{LLM} in healthcare has grown exponentially, with applications spanning patient care, research, and education.
%
In patient care, \glspl{LLM}-based chatbots can assist healthcare professionals by abstracting key results from literature, detecting medical errors, and supporting clinical decisions.
%
For patients, these chatbots can provide trustworthy and empathetic answers, resembling a dialogue with a physician, and proactively suggest actions based on tracked activities and vital signs.
%
In research, \glspl{LLM} can automate tasks such as data analysis, summarization, and literature search.
%
In education, they can serve as interactive tutors, providing teaching material and demonstrating strong performance in medical examinations.

%
\subsubsection{Challenges and Techniques}
%
Three key requirements must be addressed for deploying \glspl{LLM} in healthcare.
%
First, ethical concerns, including privacy and security risks, must be mitigated~\cite{lancet2023}.
%
Second, the system must be reliable, avoiding hallucinations and ensuring accuracy~\cite{EthicsLLMs-NatureDigitalMedicine2024}.
%
Third, the chatbot must communicate empathetically, motivating patients and providing real-time support~\cite{Chatbot-JAMAIntMed2023}.
%
To meet these requirements, open-source \glspl{LLM} are preferred, as they allow local deployment and direct access to model weights.
%
However, open models may generate irrelevant or incomplete content, undermining trust and compliance.
%
To address these limitations, two primary techniques are recommended: \gls{RAG} and fine-tuning.
%
While fine-tuning aligns models with domain-specific datasets, \gls{RAG} dynamically updates the knowledge base, ensuring relevance to recent information.

%
\subsubsection{\Gls{RAG} in the Medical Domain}
%
\Gls{RAG} represents an innovative integration of information retrieval and generative models, enabling access to a medical knowledge base for generating precise and contextually relevant responses.
%
This approach enhances the safety and effectiveness of \glspl{LLM} in healthcare, where accuracy and specificity directly impact patient care quality.
%
For example, \gls{RAG} has been used to improve \glspl{LLM} in digestive diseases~\cite{Giuf2024} and Hepatitis C management~\cite{Kresevic2024-NPJ}, outperforming baseline models in delivering guideline-specific recommendations.
%
However, implementing \gls{RAG} in healthcare requires tailoring solutions to local needs, considering factors such as demographics, resources, and cultural practices.
%
This flexibility necessitates a modular and configurable architecture.

%
\subsubsection{Fine-Tuning in the Medical Domain}
%
Fine-tuning has demonstrated impressive results in specialized medical domains~\cite{Maharjan2024-ScientificReports}.
%
For instance, Wang et al.~\cite{Wang2023-Jamia} fine-tuned the Llama 2 model using clinical concepts from standardized vocabularies, such as the Human Phenotype Ontology (HPO), to address underdiagnosis and misdiagnosis.
%
However, due to the limited dataset size in this study, we focus on \gls{RAG}, which allows dynamic updates to the knowledge base, ensuring the model remains relevant with recent information.


\subsection{Materials and Methods}
\label{subsec:materials-and-methods-rag}

\subsubsection{Fine-Tuning}
\glspl{LLM} are initially pre-trained on large, general-purpose text corpora to predict the next token in a sequence.
%
This process equips the model with a broad understanding of language and serves as a foundation for fine-tuning.
%
Fine-tuning involves further training the model on smaller, domain-specific datasets to adapt it to specific tasks or fields.
%
This approach is computationally efficient as it leverages the model's pre-existing knowledge while refining it for specialized applications.

Full fine-tuning updates all the model's parameters to optimize its performance for a specific task.
%
While effective, this method is resource-intensive, requiring significant computational power and memory.

Parameter-efficient fine-tuning (PEFT) updates only a subset of the model's parameters, freezing the rest.
%
This reduces memory requirements while retaining the model's general linguistic knowledge.
%
Techniques such as low-rank adaptation (LoRA) and quantized LoRA (QLoRA) further minimize resource usage by refining smaller weight matrices and lowering the precision of adapter weights~\cite{DBLP:conf/iclr/HuSWALWWC22,DBLP:conf/nips/DettmersPHZ23}.
%
By selecting the appropriate fine-tuning method, models can be optimized for specific use cases while minimizing resource consumption.

\subsubsection{\gls{RAG}}
%
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/rag-architecture}
    \caption[\Gls{RAG} architecture]{
        \Gls{RAG} architecture.
        %
        The retriever identifies relevant information from a knowledge base, which the generator uses to produce a contextually appropriate response.
    }
    \label{fig:rag-architecture}
\end{figure}
%
Traditionally, \glspl{LLM} generate responses based solely on patterns and information learned during pre-training.
%
This limitation often results in responses lacking depth or specific knowledge.
%
\gls{RAG} addresses this by integrating external data during the response generation process.

The \gls{RAG} framework consists of two main phases:
%
\begin{enumerate}
    \item Retrieving relevant information from a large dataset or knowledge base in response to a query.
    %
    \item Using the retrieved information to guide the generation of the response.
\end{enumerate}
%
This enables software agents, such as chatbots, to provide accurate and context-specific answers by supplementing the model's internal knowledge with external information, such as private documentation or databases.

The retriever component identifies relevant information by splitting documents into smaller fragments (chunks) and converting these chunks into embedding vectors.
%
These vectors are stored in an indexed knowledge base for efficient retrieval.
%
When a query is processed, the system generates a query vector and matches it with stored document vectors using vector similarity techniques.
%
Sparse embeddings, such as those based on TF-IDF or BM25~\cite{DBLP:books/cu/LeskovecRU14,DBLP:journals/ftir/RobertsonZ09}, rely on keyword matches but may struggle with synonyms and semantic meaning.
%
Dense embeddings, generated by models like \gls{BERT}, capture deeper semantic relationships, enabling retrieval based on meaning rather than exact words~\cite{DBLP:conf/naacl/DevlinCLT19}.
%
Hybrid approaches combine both methods to balance speed and semantic depth.
%
The generator component, a \gls{LLM}, produces the final text response.
%
\Cref{fig:rag-architecture} illustrates the \gls{RAG} architecture, highlighting the interaction between the retriever and generator components.


\subsubsection{\gls{RAG} vs. Fine-Tuning}
\gls{RAG} and fine-tuning represent distinct approaches to enhancing \glspl{LLM}, each with unique advantages.
%
\begin{itemize}
    \item \textbf{Knowledge Integration vs. Task Specialization:} \gls{RAG} integrates external knowledge dynamically, enhancing versatility and ensuring up-to-date information.
    %
    Fine-tuning specializes the model for specific tasks, improving task-specific accuracy.
    %
    \item \textbf{Dynamic vs. Static Learning:} \gls{RAG} allows dynamic access to external data, while fine-tuning updates the model based on a static training cycle.
    %
    \item \textbf{Generalization vs. Customization:} \gls{RAG} preserves the model's generality, making it adaptable across tasks, whereas fine-tuning tailors the model for specific use cases.
    %
    \item \textbf{Resource Demands:} \gls{RAG} requires runtime retrieval mechanisms, while fine-tuning is resource-intensive during training but efficient during deployment.
\end{itemize}

\subsubsection{Chosen Approach}
%
\input{tables/llms-rag}
%
This study employs \gls{RAG} techniques to enhance the performance of open \glspl{LLM} for developing a chatbot supporting hypertensive patients.
%
\gls{RAG} was chosen over fine-tuning for several reasons:
%
\begin{itemize}
    \item It dynamically integrates external data, allowing the system to adapt to diverse contexts and provide accurate information.
    %
    \item Fine-tuning requires extensive computational resources, which are currently unavailable.
    %
    \item \gls{RAG} mitigates issues such as model drift, hallucinations, and biases that can arise during fine-tuning.
\end{itemize}

\paragraph{Retrieval Strategies}
Three retrieval strategies were tested:
%
\begin{itemize}
    \item \textbf{Base Retriever:} Retrieves information based on vector similarity using maximum marginal relevance~\cite{DBLP:conf/sigir/CarbonellG98}.
    %
    \item \textbf{Multi-Query Retriever:} Enhances diversity and relevance by generating multiple queries with a \gls{LLM} and retrieving relevant chunks for each query.
    %
    \item \textbf{Ensemble Retriever:} Combines outputs from multiple retrievers, such as the base retriever and BM25, using reciprocal rank fusion~\cite{DBLP:conf/sigir/CormackCB09}.
\end{itemize}

The dataset used for \gls{RAG} consists of 1,473 question-answer pairs extracted from medical consultations.
%
The questions cover topics related to hypertension, including symptoms, causes, treatments, and lifestyle recommendations.
%
All answers, written in Italian, were reviewed by medical professionals to ensure accuracy and relevance.

The evaluated \glspl{LLM} include both general-purpose models (e.g., Llama 3.1, Qwen2, Mistral Nemo) and medical-domain-specific models (e.g., Llama 3.1-Medical, Qwen2-Medical).
%
Further details are provided in Table~\ref{tab:evaluated-llms}.


\subsection{Evaluation}
\label{subsec:evaluation-rag}

\subsubsection{Experimental Setup}
We evaluated the effectiveness of various \gls{RAG} techniques using the RAGAS framework\footnote{\url{https://docs.ragas.io/en/stable/}}.
%
This framework provides a comprehensive suite of metrics for assessing retrieval and generation aspects of \gls{LLM}-based systems.
%
Instead of partitioning the dataset into training and test sets, we leveraged an external \gls{LLM} (GPT-4o) to generate a test set of 20 question-context-answer triplets.
%
These triplets were designed to maintain statistical relevance to the original dataset.
%
We evaluated a range of state-of-the-art open-source \glspl{LLM}, both general-purpose and domain-specific, with and without \gls{RAG}.
%
The evaluation employed the following prompt:
%
\begin{promptbox}[Medical system prompt]
    \scriptsize
    You are an AI medical assistant specializing in hypertension.
    Provide detailed and evidence-based answers, using clear and accessible language. Always respect patient privacy, and if you are unsure of the answer,
    state ``I am not sure of the answer.''
    Base your response on the provided context to answer accurately.
    Include current recommendations and explain medical concepts in an understandable way.

    **Context:** { context }

    **Question:** { question }
\end{promptbox}
%
In the case of \gls{RAG}, the context was the retrieved information from the knowledge base.
%
Otherwise, the context was left empty.


\subsubsection{Metrics}
To evaluate the performance of the \gls{RAG} systems, we employed three key metrics: \textit{Answer Relevancy}, \textit{Answer Correctness}, and \textit{Faithfulness}.
%
These metrics were computed using a reference \gls{LLM} (GPT-4o) to analyze the quality of the generated responses.

\paragraph{Answer Relevancy}
This metric measures the alignment between the generated response and the original question.
%
It is computed as the cosine similarity between the embedding of the original question, \(E_o\), and the embeddings of the generated responses, \(E_{g_i}\), for \(i \in \{1, \dots, N\}\):
%
\begin{equation}
    \label{eq:answer-relevancy}
    \text{Answer Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \cos(E_{g_i}, E_o) = \frac{1}{N} \sum_{i=1}^{N} \frac{E_{g_i} \cdot E_o}{\|E_{g_i}\| \|E_o\|}.
\end{equation}
%
Higher values indicate greater relevance.

\paragraph{Answer Correctness}
This metric, denoted as \(AC\), evaluates the factual accuracy of a generated answer \(A\) with respect to a ground truth answer \(G\).
%
It combines \textit{Factual Correctness} (\(FC\)) and \textit{Semantic Similarity} (\(SS\)), both ranging from 0 to 1:
%
\begin{equation}
    \label{eq:answer-correctness}
    AC = w_1 \cdot FC + w_2 \cdot SS, \quad w_1 + w_2 = 1.
\end{equation}
%
The \(FC\) is computed using the \(F_1\)-score:
%
\begin{equation}
    \label{eq:factual-correctness}
    FC = \frac{|TP|}{|TP| + 0.5 \cdot (|FP| + |FN|)},
\end{equation}
%
where \(TP\), \(FP\), and \(FN\) represent true positives, false positives, and false negatives, respectively.
%
The \(SS\) is calculated as the cosine similarity between the embeddings of \(A\) and \(G\).

\paragraph{Faithfulness}
This metric assesses the consistency of the generated answer \(A\) with the provided context \(C\).
%
Let \(C_A = \{c_1, c_2, \dots, c_n\}\) be the set of claims in \(A\).
%
The faithfulness score \(F\) is defined as:
%
\begin{equation}
    \label{eq:faithfulness}
    F(A, C) = \frac{|\{c_i \in C_A \mid c_i \text{ can be inferred from } C\}|}{n}.
\end{equation}
%
Higher scores indicate greater factual consistency.

\subsubsection{Results}
%
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \includegraphics[width=\textwidth]{figures/RAGvsNoRAG_correctness}
        \caption{Answer Correctness}
        \label{fig:ragvsnorag_correctness}
    \end{subfigure}
        \begin{subfigure}{0.9\textwidth}
        \includegraphics[width=\textwidth]{figures/RAGvsNoRAG_relevancy}
        \caption{Answer Relevancy}
        \label{fig:ragvsnorag_relevancy}
    \end{subfigure}
    \caption[Performance comparison of \gls{RAG}-based systems against plain \gls{LLM} systems]{
      Performance comparison of \gls{RAG}-based systems against plain \gls{LLM} systems.
      %
      (a) Answer Correctness, (b) Answer Relevancy.
      %
      The results demonstrate that \gls{RAG} significantly enhances both answer correctness and relevancy across all tested models.
    }
    \label{fig:ragvsnorag}
\end{figure}
%
Our experiments revealed that \gls{RAG}-based systems consistently outperformed plain \glspl{LLM} in terms of \textit{Answer Relevancy} and \textit{Answer Correctness} as shown in \Cref{fig:ragvsnorag}.
%
The performance boost was particularly significant for models trained on medical data, with improvements of up to 20\% in correctness and 40\% in relevancy.

\paragraph{Impact of Retrieval Strategies}
%
\begin{figure}[h]
  \centering
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/heatmap_Base}
    \caption{Base Retriever}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/heatmap_Ensemble}
    \caption{Ensemble Retriever}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/heatmap_Multi}
    \caption{MultiQuery Retriever}
  \end{subfigure}
  \caption[Performance comparison of different retrieval strategies]{
      Performance comparison of different retrieval strategies: (a) Base Retriever, (b) Ensemble Retriever, (c) MultiQuery Retriever.
      %
      Each strategy demonstrates strengths and weaknesses across various evaluation metrics, highlighting the need for careful selection based on the specific task and model.
  }
  \label{fig:retrievers}
\end{figure}
%
The choice of retrieval strategy significantly influenced performance.
%
As illustrated in \Cref{fig:retrievers}, each strategy exhibited distinct strengths and weaknesses across the evaluation metrics.
%
The \textit{Base Retriever} excelled in correctness and faithfulness, while the \textit{Ensemble Retriever} achieved superior relevancy.
%
The \textit{Multi-Query Retriever} showed promise in relevancy but struggled with correctness due to the inclusion of irrelevant information.

\paragraph{Domain-Specific Fine-Tuning}
%
Specialized models, such as LlamaMed and Qwen2-Med, outperformed their general-purpose counterparts in relevancy (up to 5\%) and correctness (up to 3\%).
%
However, \gls{RAG}-augmented base models consistently surpassed even specialized models without \gls{RAG}, highlighting the advantages of retrieval augmentation.

\subsubsection{Discussion}
The findings demonstrate the effectiveness of \gls{RAG} as a robust tool for enhancing open-source \glspl{LLM} in the medical domain.
%
By incorporating a verified knowledge base, \gls{RAG} significantly improves the accuracy and relevance of chatbot responses, making it an optimal solution for supporting hypertensive patients.
%
Even without large datasets for fine-tuning, \gls{RAG} provides an efficient, out-of-the-box solution for creating high-performance chatbots in specialized domains.
%
Future work should include human evaluation by medical professionals to assess clinical accuracy and explore metrics for evaluating empathetic responses.