%! Author = matteomagnini
%! Date = 05/03/25

%----------------------------------------------------------------------------------------
\chapter{Introduction}
\label{ch:introduction}
\mtcaddchapter
\minitoc
%----------------------------------------------------------------------------------------

\begin{refsection}

\section{Research background and context}
\label{sec:research-background-and-context}
%
Through the course of history, humanity has experienced several socio-technological revolutions that have changed the way we live.
%
From the first industrial revolution that initiated the automation of manual labor, to the world-wide spread of computers that started the automation of processes and decision-making \sidenote{cite/talk about expert systems(?)}, we are now witnessing the \ac{AI} revolution.
%
The advent of \ac{AI} has already successfully automated cognitive tasks \sidenote{add citation to image recognition and similar}, and it is expected to go further by reaching -- and possibly surpassing -- \emph{human-level intelligence}.
%
\Ac{AI} is not a recent invention, it has been around since the 1950s.
%
The reasons why only now (in the last decade to be more precise) \ac{AI} has become ubiquitous are the presence of crucial ingredients that were missing in the past.
%
Thanks to
%
\begin{inlinelist}
    \item the enormous amount of \emph{data},
    %
    \item the improvement of \emph{memory} and \emph{computational power} -- that still follows the Moore's law --, and
    %
    \item the affordability of huge quantity of \emph{energy},
    %
\end{inlinelist}
%
\ac{AI} finally flourished again.


The first kind of \ac{AI} that was developed is \emph{symbolic}.
%
Symbolic means that there are \emph{symbols} with specific \emph{meanings} that are manipulated by algorithms.
%
Symbolic \ac{AI} follows the \emph{deductive} process of reasoning, where the system starts from a set of axioms and applies rules.
%
These kinds of \ac{AI} programs are pretty effective in well-defined domains where there are clear rules that always hold, e.g., board games,~\ac{TSP},~\acp{BWP}, etc.
%
\emph{Sub-symbolic} \ac{AI} is based on the \emph{inductive} process of reasoning.
%
Conversely to symbolic \ac{AI}, sub-symbolic \ac{AI} does not rely on symbols that have meanings for humans, but on data \emph{patterns}.
%
Programs that uses sub-symbolic \ac{AI} to solve a certain task are said to perform \ac{ML}, because a model needs to first learn from examples before being able to generalize to unseen data.
%
\sidenote{consider to add a sentence or two about other different kinds of algorithms based on different learning principles like RL}
%
Sub-symbolic models like \acp{NN} can reach \emph{super-human performance} in pre-defined tasks like image recognition, natural language processing, etc., but they require a huge amount of data and hardware resources to be trained.


The natural evolution in \ac{AI} research is to use both symbolic and sub-symbolic approaches together in order to increase the performance and face more challenging tasks.
%
This is the idea behind \emph{\ac{NeSy} \ac{AI}}, where the deductive reasoning of symbolic \ac{AI} is combined with the inductive learning of sub-symbolic models, especially \acp{NN}.
%
This branch of \ac{AI} is relatively young; the first works that combined logic rules within a \ac{NN} date back to the 90s~\cite{DBLP:conf/aaai/TowellSN90,DBLP:journals/ai/TowellS94}.
%
The last past years have been quite prolific both in the design of new \ac{NeSy} techniques and in the development of intelligent systems that use them~\cite{DBLP:journals/csur/CiattoSAMO24}.


Finally, the advent of \acp{LLM} has further transformed the landscape of \ac{AI}, offering unprecedented capabilities for natural language (and also multimodal data) generation.
%
\Acp{LLM} are huge \ac{NN} models up to \emph{hundreds of billions} of parameters that are trained on a large corpus of text data.
%
Despite the outstanding performance that \acp{LLM} have achieved in many tasks, their output is just a probability distribution over the vocabulary, therefore it is subject to errors (e.g., \emph{hallucinations}) and biases (e.g., from training data, from prompt engineering).
%
\Acp{LLM} are still a great resource for \ac{NeSy} \ac{AI} because of their performance, versatility and customizability.
%
Ultimately, the rapid progress in \ac{NeSy} \ac{AI} and the dazzling evolution of \acp{LLM} are significantly changing our world, leading to more and more intelligent systems, and possibly to the advent of the \emph{singularity}~\cite{shanahan2015technological}.


\section{Overview and contributions}
\label{sec:overview-and-contributions}
%
Engineering \ac{AI} systems with characteristics of (super-)human level of intelligence is a challenging task that requires to solve many sub-problems.
%
Human beings are able to perform \emph{deductive} and \emph{inductive reasoning}, \emph{plan}, \emph{adapt} to changes, \emph{collaborate} between each others, \emph{self organise}, \emph{learn new concepts} and much more.
%
Each of these skills contributes to defining the overall intelligence of a human being.
%
So, it follows that some of these skills -- if not all -- should be present in an \ac{AI} system in order to be considered intelligent at a human-like level.
%
In particular, the activity of (autonomously) \emph{learning} is crucial in order to reach what in research is called \ac{AGI}.


The journey to \ac{AGI} -- i.e., the development of an \ac{AI} system that is able to perform (possibly) any task -- is still far to be completed.
%
The roadmap that we propose follows a bottom-up approach.
%
\sidenote{ponder to add an image of the roadmap. In that case all the items in the nodes should be introduced also in the section}
%
First, \ac{AGI} is decomposed into a dependency graph of sub-problems and the elementary building blocks are identified (a.k.a., nodes with no dependencies).
%
Then, starting from those nodes, an \ac{SLR} is performed to identify the state of the art and common practices in solving those problems.
%
At the same time, the design and implementation of technological solutions are carried out.
%
Incrementally, once a sub-problem has been sufficiently explored, the next one is tackled.


\Ac{ML} models, such as \acp{NN}, are able to learn from data and generalize to unseen data.
%
So, in other words, they are able to perform \emph{inductive reasoning}.
%
On the other hand, symbolic \ac{AI} draws on logic rules to perform \emph{deductive reasoning}.
%
Humans are able to perform both kinds of reasoning, switching and possibly combining between them.
%
To let symbolic and sub-symbolic \ac{AI} systems ``talk'' to each other -- and therefore work together -- it is possible to use \emph{symbolic knowledge}.
%
Symbolic knowledge is a general term to refer to any kind of knowledge provided in a \emph{symbolic} form, e.g., logic rules, ontologies, free text, etc.


\Ac{SKI} is the process of \emph{injecting} prior symbolic knowledge into sub-symbolic models.
%
In this way, the sub-symbolic model is able to leverage the prior domain expertise to improve its performance.
%
Conversely, \ac{SKE} is the dual process of \emph{extracting} symbolic knowledge from sub-symbolic models.
%
By doing so, the knowledge learned by the model from the data is made available to both humans and other symbolic systems.
%
\Ac{SKI} and \ac{SKE} are the pillars upon which advanced \ac{NeSy} \ac{AI} systems can be built.


\subsection*{Research questions}
%
\begin{questions}
    \item \emph{What are the characteristics of \ac{SKI} and \ac{SKE} techniques?}

    There are different ways to perform \ac{SKI} and \ac{SKE}, depending on multiple dimensions.
    %
    From these dimensions -- such as the kind of supported sub-symbolic models, the formalism of the symbolic knowledge, the ways to inject/extract the knowledge, etc. -- it is possible to refine the main research question into more detailed sub-questions.
    %
    Ultimately, from the answers to these research questions it would be possible to define a comprehensive taxonomy of \ac{SKI} and \ac{SKE} techniques.
    %
    \label{itm:rq1}

    \item \emph{How can the effects of \ac{SKI} and \ac{SKE} be measured?}

    Accuracy and other popular metrics in \ac{ML} are not the only ones that should be considered when evaluating \ac{SKI} and \ac{SKE} techniques.
    %
    There are many other aspects that a scientist or the final user of the technology wants to know.
    %
    For instance,
    %
    how much robust is the model with injected knowledge to data degradation,
    %
    how much the extracted knowledge is aligned with the actual behavior of the model,
    %
    is it possible to reduce the size of the model by injecting knowledge without losing performance, and so on.
    %
    \label{itm:rq2}

    \item \emph{When and where should \ac{SKI} and \ac{SKE} be used?}

    Traditionally, \ac{SKE} is born in the context of \ac{XAI}, where the objective is to provide a human-understandable explanation of the model's behavior.
    %
    \Ac{SKI}, on the other hand, is born to improve the performance of the model.
    %
    However, for both of them there are many other possible applications.
    %
    \label{itm:rq3}

    \item \emph{How to design and develop \ac{NeSy} \ac{AI} systems that leverage \ac{SKI} and \ac{SKE}?}

    The use of \ac{SKI} and \ac{SKE} enables a variety of new applications and research directions.
    %
    These new possibilities must be explored taking into account all the consequences and implications of the use of these techniques.
    %
    \label{itm:rq4}
\end{questions}


\subsection*{Contributions}
%
The thesis mainly contributes to the field of \ac{NeSy} \ac{AI} and software engineering.
%
In particular, it focuses on \ac{SKI} and \ac{SKE} methods and on the development of \ac{AI} systems that leverage them.
%
The \textit{fil rouge} that binds all the contributions is the goal to design and develop intelligent systems, ultimately with capabilities of \emph{autonomous learning}.
%
The contributions are manifold, and they cover different aspects of \ac{NeSy} \ac{AI} including: \ac{SKI} and \ac{SKE}, software engineering, and social-technical systems.
%
In this thesis,
%
\begin{enumerate}[label=\emph{(\roman*)}]
    \item \textbf{\ac{SKI} and \ac{SKE}}

    \begin{enumerate}[label=\emph{(\arabic*)},resume]
        %
        \item we systematically collect and organise into a taxonomy \ac{SKI} and \ac{SKE} methods and technologies (\Cref{itm:rq1});
        %
        \item we design, implement and validate new \ac{SKI} and \ac{SKE} methods (\Cref{itm:rq1,itm:rq3});
        %
        \item we define new metrics to evaluate the performance of \ac{SKI} techniques (\Cref{itm:rq2,itm:rq3}).
        %
    \end{enumerate}
    %
    \item \textbf{Software engineering}

    \begin{enumerate}[label=\emph{(\arabic*)},resume]
        %
        \item we design and develop software libraries to support the development and integration of \ac{SKI} and \ac{SKE} methods in \ac{AI} systems (\Cref{itm:rq4});
        %
        \item we design and develop \ac{NeSy} \ac{AI} systems that leverage \ac{SKI} and \ac{SKE} techniques in real-world scenarios (\Cref{itm:rq3,itm:rq4});
        %
    \end{enumerate}
    %
    \item \textbf{Social-technical systems}

    \begin{enumerate}[label=\emph{(\arabic*)},resume]
        %
        \item we investigate how \ac{SKI} techniques can be used to mitigate the risk of bias in \ac{AI} systems (\Cref{itm:rq3,itm:rq4});
        %
    \end{enumerate}
    %
\end{enumerate}


\section{Structure of the thesis}
\label{sec:structure-of-the-thesis2}
%
This thesis is structured as follows.
%
\Cref{ch:introduction} sets the table, providing the background and context of the research, the research questions, and the contributions.
%
It also provides an overview of how the thesis is organised.


\Cref{part:background} gives the background necessary to understand all aspects of the research.
%
In \Cref{ch:intelligent-systems} where we introduce the broad topic of \emph{intelligence}, and we show its declinations in living beings and machines.
%
A considerable part of the chapter is dedicated to the learning process, which is one of the main focus of this thesis.
%
\Cref{ch:ai} introduces the reader to the field of \ac{AI}, where we present \emph{symbolic} and \emph{sub-symbolic} \ac{AI}.
%
\Cref{ch:nesy-ai} follows with a presentation of \ac{NeSy} \ac{AI}, with particular focus to \ac{SKI} and \ac{SKE}.
%
Finally, \Cref{ch:llm} brings the attention to \acp{LLM}, which are the latest -- and still ongoing -- breakthrough in \ac{AI}.




\label{sec:structure-of-the-thesis}
%
Lorem Ipsum

\printbibliography[title=Reference,heading=bibintoc]

\end{refsection}