%! Author = matteomagnini
%! Date = 05/03/25

%----------------------------------------------------------------------------------------
\chapter{Introduction}
\label{ch:introduction}
\mtcaddchapter
\minitoc
%----------------------------------------------------------------------------------------

\begin{refsection}

\section{Research background and context}
\label{sec:research-background-and-context}
%
Through the course of history, humanity has experienced several socio-technological revolutions that have changed the way we live.
%
From the first industrial revolution that initiated the automation of manual labor, to the world-wide spread of computers that started the automation of processes and decision-making \sidenote{cite/talk about expert systems(?)}, we are now witnessing the \ac{AI} revolution.
%
The advent of \ac{AI} has already successfully automated cognitive tasks \sidenote{add citation to image recognition and similar}, and it is expected to go further by reaching -- and possibly surpassing -- \emph{human-level intelligence}.
%
\Ac{AI} is not a recent invention, it has been around since the 1950s.
%
The reasons why only now (in the last decade to be more precise) \ac{AI} has become ubiquitous are the presence of crucial ingredients that were missing in the past.
%
Thanks to
%
\begin{inlinelist}
    \item the enormous amount of \emph{data},
    %
    \item the improvement of \emph{memory} and \emph{computational power} -- that still follows the Moore's law --, and
    %
    \item the affordability of huge quantity of \emph{energy},
    %
\end{inlinelist}
%
\ac{AI} finally flourished again.


The first kind of \ac{AI} that was developed is \emph{symbolic}.
%
Symbolic means that there are \emph{symbols} with specific \emph{meanings} that are manipulated by algorithms.
%
Symbolic \ac{AI} follows the \emph{deductive} process of reasoning, where the system starts from a set of axioms and applies rules.
%
These kinds of \ac{AI} programs are pretty effective in well-defined domains where there are clear rules that always hold, e.g., board games,~\ac{TSP},~\acp{BWP}, etc.
%
\emph{Sub-symbolic} \ac{AI} is based on the \emph{inductive} process of reasoning.
%
Conversely to symbolic \ac{AI}, sub-symbolic \ac{AI} does not rely on symbols that have meanings for humans, but on data \emph{patterns}.
%
Programs that uses sub-symbolic \ac{AI} to solve a certain task are said to perform \ac{ML}, because a model needs to first learn from examples before being able to generalize to unseen data.
%
\sidenote{consider to add a sentence or two about other different kinds of algorithms based on different learning principles like RL}
%
Sub-symbolic models like \acp{NN} can reach \emph{super-human performance} in pre-defined tasks like image recognition, natural language processing, etc., but they require a huge amount of data and hardware resources to be trained.


The natural evolution in \ac{AI} research is to use both symbolic and sub-symbolic approaches together in order to increase the performance and face more challenging tasks.
%
This is the idea behind \emph{\ac{NeSy} \ac{AI}}, where the deductive reasoning of symbolic \ac{AI} is combined with the inductive learning of sub-symbolic models, especially \acp{NN}.
%
This branch of \ac{AI} is relatively young; the first works that combined logic rules within a \ac{NN} date back to the 90s~\cite{DBLP:conf/aaai/TowellSN90,DBLP:journals/ai/TowellS94}.
%
The last past years have been quite prolific both in the design of new \ac{NeSy} techniques and in the development of intelligent systems that use them~\cite{DBLP:journals/csur/CiattoSAMO24}.


Finally, the advent of \acp{LLM} has further transformed the landscape of \ac{AI}, offering unprecedented capabilities for natural language (and also multimodal data) generation.
%
\Acp{LLM} are huge \ac{NN} models up to \emph{hundreds of billions} of parameters that are trained on a large corpus of text data.
%
Despite the outstanding performance that \acp{LLM} have achieved in many tasks, their output is just a probability distribution over the vocabulary, therefore it is subject to errors (e.g., \emph{hallucinations}) and biases (e.g., from training data, from prompt engineering).
%
\Acp{LLM} are still a great resource for \ac{NeSy} \ac{AI} because of their performance, versatility and customizability.
%
Ultimately, the rapid progress in \ac{NeSy} \ac{AI} and the dazzling evolution of \acp{LLM} are significantly changing our world, leading to more and more intelligent systems, and possibly to the advent of the \emph{singularity}~\cite{shanahan2015technological}.


\section{Overview and contributions}
\label{sec:overview-and-contributions}
%
Engineering \ac{AI} systems with characteristics of (super-)human level of intelligence is a challenging task that requires to solve many sub-problems.
%
Human beings are able to perform \emph{deductive} and \emph{inductive reasoning}, \emph{plan}, \emph{adapt} to changes, \emph{collaborate} between each others, \emph{self organise}, \emph{learn new concepts} and much more.
%
Each of these skills contributes to defining the overall intelligence of a human being.
%
So, it follows that some of these skills -- if not all -- should be present in an \ac{AI} system in order to be considered intelligent at a human-like level.
%
In particular, the activity of (autonomously) \emph{learning} is crucial in order to reach what in research is called \ac{AGI}.


The journey to \ac{AGI} -- i.e., the development of an \ac{AI} system that is able to perform (possibly) any task -- is still far to be completed.
%
The roadmap that I propose follows a bottom-up approach.
%
\sidenote{ponder to add an image of the roadmap. In that case all the items in the nodes should be introduced also in the section}
%
First, \ac{AGI} is decomposed into a dependency graph of sub-problems and the elementary building blocks are identified (a.k.a., nodes with no dependencies).
%
Then, starting from those nodes, an \ac{SLR} is performed to identify the state of the art and common practices in solving those problems.
%
At the same time, the design and implementation of technological solutions are carried out.
%
Incrementally, once a sub-problem has been sufficiently explored, the next one is tackled.


\Ac{ML} models, such as \acp{NN}, are able to learn from data and generalize to unseen data.
%
So, in other words, they are able to perform \emph{inductive reasoning}.
%
On the other hand, symbolic \ac{AI} draws on logic rules to perform \emph{deductive reasoning}.
%
Humans are able to perform both kinds of reasoning, switching and possibly combining between them.
%
To let symbolic and sub-symbolic \ac{AI} systems ``talk'' to each other -- and therefore work together -- it is possible to use \emph{symbolic knowledge}.
%
Symbolic knowledge is a general term to refer to any kind of knowledge provided in a \emph{symbolic} form, e.g., logic rules, ontologies, free text, etc.


\Ac{SKI} is the process of \emph{injecting} prior symbolic knowledge into sub-symbolic models.
%
In this way, the sub-symbolic model is able to leverage the prior domain expertise to improve its performance.
%
Conversely, \ac{SKE} is the dual process of \emph{extracting} symbolic knowledge from sub-symbolic models.
%
By doing so, the knowledge learned by the model from the data is made available to both humans and other symbolic systems.
%
\Ac{SKI} and \ac{SKE} are the two fundamental pillars upon which more advanced \ac{NeSy} \ac{AI} systems can be built.


\subsection*{Research questions}
%
\begin{itemize}
    \item[\textbf{RQ1}:]\label{itm:rq1} \emph{What are the characteristics of \ac{SKI} and \ac{SKE} techniques?}

    There are different ways to perform \ac{SKI} and \ac{SKE}, depending on multiple dimensions.
    %
    From these dimensions -- such as the kind of supported sub-symbolic models, the formalism of the symbolic knowledge, the ways to inject/extract the knowledge, etc. -- it is possible to refine the main research question into more detailed sub-questions.
    %
    Ultimately, from the answers to these research questions it would be possible to define a comprehensive taxonomy of \ac{SKI} and \ac{SKE} techniques.
    %
    \item[\textbf{RQ2}:]\label{itm:rq2} \emph{How can the effects of \ac{SKI} and \ac{SKE} be measured?}

    Accuracy and other popular metrics in \ac{ML} are not the only ones that should be considered when evaluating \ac{SKI} and \ac{SKE} techniques.
    %
    There are many other aspects that a scientist or the final user of the technology want to know.
    %
    For instance, how much resilient is the model with injected knowledge to data degradation?
    %
    \item[\textbf{RQ3}:]\label{itm:rq3}
    \item[\textbf{RQ4}:]\label{itm:rq4}
\end{itemize}

\subsection*{Contributions}
%
In this thesis,
%
\begin{inlinelist}
    %
    \item I systematically collect and organise \ac{SKI} and \ac{SKE} methods and technologies,
    %
    \item I design and develop software libraries to support the development and integration of \ac{SKI} and \ac{SKE} methods in \ac{AI} systems,
    %
    \item I design, implement and validate new \ac{SKI} and \ac{SKE} methods,
    %
    \item I define new metrics to evaluate the performance of \ac{SKI} techniques,
    %
    \item I investigate how applying \ac{SKI} techniques affects the performance of the models under different data degradation scenarios, and
    %
    \item I design and develop \ac{NeSy} \ac{AI} systems that leverage \ac{SKI} and \ac{SKE} techniques in real-world scenarios.
    %
\end{inlinelist}


\section{Structure of the thesis}
\label{sec:structure-of-the-thesis}
%
Lorem Ipsum

\printbibliography[title=Reference,heading=bibintoc]

\end{refsection}