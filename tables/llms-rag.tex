\begin{table}
  \begin{tabular}{|l|p{0.7\textwidth}|}
      \hline
      \textbf{Model} & \textbf{Description} \\
      \hline
      Llama3.1 & A general-purpose \gls{LLM} based on the Llama3 architecture with 8B parameters, trained on a diverse range of text data~\cite{llama3}. \\
      \hline
      Llama3.1-Medical & A medical-domain-specific version of Llama3.1, fine-tuned on medical data to enhance its performance in healthcare applications: \url{https://ollama.com/qordmlwls/llama3.1-medical}. \\
      \hline
      Qwen2 & A general-purpose \gls{LLM} with 7B parameters, trained on 29 different languages to improve cross-lingual performance~\cite{qwen2}. \\
      \hline
      Qwen2-Medical & A medical-domain-specific version of Qwen2, fine-tuned on medical data to improve its performance in healthcare tasks: \url{https://ollama.com/echelonify/med-qwen2}. \\
      \hline
      Mistral-Nemo & A 12B parameter model with a large context window (128K tokens) developed by Nvidia: \url{https://ollama.com/library/mistral-nemo}. \\
      \hline
      Phi3 & A relatively small \gls{LLM} with 3B parameters, trained by Microsoft on filtered high-quality data~\cite{abdin2024phi3technicalreporthighly}. \\
      \hline
      Gemma2 & A 9B parameter model based on Deepmind Gemini developed by Google~\cite{gemmateam2024gemma2improvingopen}. \\
      \hline
  \end{tabular}
  \centering
  \caption{Evaluated \gls{LLM} for our study.}
  \label{tab:evaluated-llms}
\end{table}